0 (0s):
The following is a conversation with Elon Musk, his fourth time on this, the Lex Friedman podcast. I thought you were gonna finish it. It's one of the greatest themes in all of film history.

2 (31s):
Yeah, that's great.

0 (33s):
So I was just thinking about the Roman Empire. As one does, I

2 (38s):
Does that whole meme where old guys are thinking about throwing empire at least

0 (44s):
Once a day. And half the population's confused whether it's true or not, but more seriously thinking about the wars going on in the world today. And as you know, war and military conquest has been a big part of Roman society and culture. And it, I think, has been a big part of most empires and dynasties throughout human history. So yeah,

2 (1m 7s):
They usually came as a result of conquest. I mean, there's some, like the Hungarian Empire where there was just a lot of sort of clever marriages,

0 (1m 17s):
But fundamentally there's an engine of conquest. Yeah. And they celebrate always excellence in warfare. Many of the leaders were excellent generals. Yeah. That kind of thing. So a big picture question, Grok approved, I asked, this is a good question to ask it. Tested

2 (1m 33s):
Grok approved.

0 (1m 34s):
Yeah. At least on fun mode. To what degree do you think war is part of human nature versus a consequence of how human societies are structured? I ask this as you have somehow controversially been a proponent of peace.

2 (1m 57s):
I'm, I'm generally a proponent of peace. I mean, ignorance is perhaps, in my view, the real enemy to be countered. That's the real hard part. Not, you know, fighting other humans, but all, all creatures fight. I mean, the, the, the jungle is a, you look at the people think of, of this nature as perhaps some sort of peaceful thing. But in fact, it is not. There's some quite funny when a, when AZ hog thing where he is like in the jungle, like saying that it's like basically just murder and death in every direction. I mean, the, the, the plants animals in the jungle are constantly trying to kill each other every single day, every minute. So it's not like, you know, we're unusual in that respect.

0 (2m 41s):
Well, this, there's a relevant question here, whether with greater intelligence comes greater control over these base instincts for violence.

2 (2m 50s):
Yes. We have much more vulnerability to control our, our limb instinct for violence than say, a chimpanzee. And in fact, if, if one looks at, say, chimpanzee society, it is not friendly. I mean, the B Bonobos are an exception, but chimpanzee society is filled with violence. And it's quite, quite horrific, frankly, that that's, that's our Olympic system in action. Like, you don't wanna be on the wrong side of a chimpanzee, eat your face off and tear and nuts off.

0 (3m 22s):
Yeah. Basically, there's no limits or ethics or there almost had just war. There's no, just war in the chi in chimpanzee societies is, is war and, and, and dominance by any means necessary. Yeah.

2 (3m 33s):
Chimpanzee society is a pro, like a permanent version of human society. It's, they're not like peace loving basically at all. There, there's extreme violence. And then once in a while, some, some, somebody who's watched too many Disney movies decides to raise a chopan zee as a pet, and then that eats their face, or they're not so off or chew their fingers off and that kind thing. Yeah. It's happened several times.

0 (3m 58s):
Ripping your nuts off is an interesting strategy for interaction.

2 (4m 2s):
Some, it's happened to people. It's unfortunate. Like that's, I guess, a one way to ensure that the other chimp doesn't like, you know, contribute to the gene pool.

0 (4m 10s):
Well, from a martial arts perspective is the fascinating strategy.

2 (4m 15s):
The, the, the nut rougher.

0 (4m 19s):
I wonder which of the martial arts teaches that one.

2 (4m 21s):
I think it's safe to say if somebody's got your nuts in their hands, and as the option of rubbing them off, you'll be amenable to whatever they want. Yeah.

0 (4m 31s):
Safe to say, say so. Like I said, somehow controversially, you've been an proponent of peace on, on Twitter on X. Yeah. So let me ask you about the wars going on today and to see what the path to peace could be. How do you hope the current war in Israel and Gaza comes to an end? What path do you see that can minimize human suffering in the long term in that part of the world?

2 (4m 55s):
Well, I think that that part of the world is, is definitely, like, if you look up the, there is no easy answer in the dictionary. It'll be that, like the picture of the Middle East in Israel especially. So there is no easy answer or what my, this is strictly my opinion of, you know, is that the, the goal of Moss was to provoke an overreaction from Israel. They obviously did not expect to, you know, have a military victory, but they, they expect, they, they really wanted to commit the worst trustees that they could in order to provoke the, the most aggressive response possible from Israel.

2 (5m 42s):
And then leverage that aggressive response to rally Muslims worldwide for the course of Gaza and Palestine, which they have succeeded in doing. So. The, this the, the counterintuitive thing here, I think that the, the thing that I think should be done, even though it's very difficult, is that I, I would recommend that Israel engage in the most cons, conspicuous acts of kindness possible. Every po everything that is the actual thing that we thought the goal of Hamas.

0 (6m 20s):
So in some sense, the degree that makes sense in geopolitics turn the other cheek implemented.

2 (6m 26s):
It's not exactly turn the other cheek. Because I do think that there's, you know, I, I think it, it is appropriate for Israel to find the Hamas war members and, you know, either, either kill 'em or incarcerate them. Like that's something that's something has to be done 'cause that they're just gonna keep, keep, keep coming otherwise, but in addition to that, they need to do whatever they can. There's some talk of, so establishing, for example, a mobile hospital, I'd recommend doing that. Just making sure that, you know, there's food, water, medical, necessities, and, and just be over the top about it and be very transparent.

2 (7m 20s):
So it's that. So the can't bill can claim is a trick. Like, just put a webcam on the thing, you know, or 24 7

0 (7m 29s):
Deploy acts of kindness.

2 (7m 31s):
Yeah. Conspicuous acts of kindness that, that, that are unequivocal. Meaning they can't be somehow, because Hamas war will, then their response will be, oh, it's a trick. Therefore you have to counter how, how is not a trick.

0 (7m 47s):
This ultimately fights the broader, the force of hatred in the, in the region.

2 (7m 52s):
Yes. And I'm not sure who said it, it's anal saying, but an eye for the, for an eye makes everyone blind. Now that neck of the woods, they really believe in the whole eye for an eye thing. But I mean, you really have, if, if you're not gonna just outright commit genocide, like against an entire people, which obviously would not be acceptable to, to, to really shouldn't be acceptable to anyone, then you're, you're gonna leave basically a lot of people alive who subsequently, you know, hate Israel. So really the question is like, how for, for every Hamas member that you kill, how many did you create?

2 (8m 33s):
And if you create more than you killed, you've not succeeded. That's the, you know, the real situation there. And it's safe to say that if, you know, if you know, if, if you kill somebody's child in Gaza, you've, you've, you've made at least a few homeless members who will die just, just to kill an Israeli. That's the situation. So, but, but I mean, this is one of the most contentious subjects one could possibly discuss. But, but I, I think if, if the, if the goal ultimately is some sort of long-term piece, one has to be, look at this from the standpoint of over time, are there more or fewer terrorists being created?

0 (9m 27s):
Let me just linger on war.

2 (9m 29s):
Yeah. W war, safe to say, wars always existed and always will exist,

0 (9m 33s):
Always will exist,

2 (9m 34s):
Always has always has existed and always will exist.

0 (9m 37s):
I hope not. You think it always will.

2 (9m 39s):
There will always be war. This question of just how much war and, and you know what, you know, there's this, there's this, the sort of the scope and scale of war. But to, to imagine that there would not be any war in the future, I think would be a very unlikely outcome.

0 (9m 56s):
Yeah. You talked about the culture series. There's war even there.

2 (9m 59s):
Yes. It's a giant war. The first book starts off with a gigantic galactic war where trillions die trillions,

0 (10m 7s):
But it still nevertheless protects these pockets of, of flourishing some. Somehow you can have galactic war and still have pockets of flourishing.

2 (10m 18s):
Yeah. I mean it's, I guess if, if, if we are able to one day expand to, you know, full the galaxy or whatever, there will be a, a galactic war at some point,

0 (10m 31s):
The scale, I mean the scale of war has been increasing, increasing, increasing. It's like a race between the scale of suffering and the scale of flourishing.

2 (10m 39s):
Yes.

0 (10m 42s):
A lot of people seem to be using this tragedy to beat the drums of war and feed the military. Industrial Complex, do you worry about this? The people who are rooting for escalation and how can it be stopped?

2 (10m 56s):
One of the things that disco concerned me is that there are very few people alive today who actually viscerally understand the horrors of war, at least in the us. I mean, obviously there are people in on the front lines in Ukraine and Russia who understand just how terrible war is, but how many people in the west understand it? And my grandfather was in World War ii. He was severely traumatized. He was there, I think in for almost six years in the, you know, in eastern North Africa and Italy. All his friends were killed in front of him. And he would've died too, except they randomly gave some, I guess, IQ test or something.

2 (11m 36s):
And he scored very high. Now, he was not an officer, he was a, I think a corporal or a sergeant or something like that, because he didn't finish high school because he had to drop outta high school. 'cause his, his, his dad died and he had to work to support his siblings. So because he didn't graduate high school, he was not eligible for the officer corps. So, you know, he kind of got put into the cannon fodder category, basically. But then just randomly they gave him this test. He was transferred to British intelligence in London. That's where he met my grandmother. But he, he had PTSD next level, like next level.

2 (12m 18s):
I mean, just didn't talk, just didn't talk. And if you tried talking to him, he'd just tell you to shut up. And he won a bunch of medals. Never, never raed about it once. And not even he hinted nothing. I, like, I found out about it 'cause I, his military wreck was, were online. That's, that's how, how I know. So he would say like, no, no way in hell do you want to, do you wanna do that again? But how many people now he, he obviously he now, he died, you know, 20 years ago or longer, actually 30 years ago. How many people are alive that remember World War ii? Not many.

0 (12m 55s):
And the same perhaps applies to the threat of nuclear war.

2 (13m 1s):
Yeah. I mean, there are enough nuclear bombs pointed at United States to make the rubble, the radio act of Revel balance many times.

0 (13m 11s):
There's two major wars going on right now. So you talked about the threat of AGI quite a bit. But now as we sit here with the intensity of conflict going on, do you worry about nuclear war?

2 (13m 26s):
I think we shouldn't discount the possibility of nuclear war. It is a civilizational threat right now. I could be wrong, but I think the, the, the current probability of nuclear war is quite low. But there are a lot of nukes pointed at us. So, and we have a lot of nukes pointed at other people. They're still there. Nobody's put their, their guns away. The missiles are still in the silos.

0 (13m 58s):
And the leaders don't seem to be the ones with the nukes talking to each other.

2 (14m 3s):
No. There are wars which are tragic and difficult on a, on a local basis. And then there are wars which are civilization ending or has that potential. Obviously global thermonuclear warfare has high potential to end civilization, perhaps, perhaps permanently. But certainly, you know, to severely wound and, and perhaps setback human progress by, you know, to the stone age or something, I don't know, pretty bad probably as scientists and engineers wanna be super popular after that as well.

2 (14m 43s):
And like you got us into this mess. So generally we, I think we, we obviously wanna prioritize civilizational risks over things that are painful and tragic on, on a local level, but not civilizational.

0 (15m 0s):
How do you hope the War in Ukraine comes to an end? And what's the path, once again to minimizing human suffering there?

2 (15m 9s):
Well, I think that what, what is likely to happen, which is really pretty much the, the way it is, is that something very close to the current lines will be how a ceasefire or truth happens. But, you know, you, you just have a situation right now where whoever goes on the offensive will suffer casualties at several times the rate of whoever's on the defense. 'cause you've got defense in depth, you've got minefields trenches, anti-tank defenses. Nobody has air superiority.

2 (15m 50s):
'cause the, the, the anti-aircraft missiles are really far better than the air the aircraft. Like, they're far more of them. And so neither side has air superiority. Tanks are basically death drafts, just slow moving. And they're, they're not immune to anti-tank weapons. So you, you really just have long range artillery and infantry ranges. It's World War I all over again with drones, you know, throwing old drones, some, some drones there,

0 (16m 25s):
Which makes the long range artillery just that much more accurate and better and so more efficient at murdering people on both sides.

2 (16m 34s):
Yeah. So it's who, who, whoever is, you don't, you don't, you don't wanna be trying to advance from either side. 'cause the quality of dying is incredibly high. So in order to overcome defense in depth, trenches and minefields, you really need a significant local superiority in numbers. Ideally combined alms where, where you, you do a fast attack with aircraft, a a concentrated number of tanks and a lot of people, that's the only way you're gonna punch through a line. And then you're gonna punch through and, and, and then not have reinforcements just kick you right out again.

2 (17m 20s):
I mean, if I, I really recommend people read World War I warfare in detail. That's rough. I mean, the sheer number of people that died there was mind boggling.

0 (17m 37s):
And it's almost impossible to imagine the end of it that doesn't look like almost exactly like the beginning in terms of what land belongs to who and so on. But on the other side of a lot of human suffering death Yes. And destruction of infrastructure.

2 (17m 56s):
Yes. I mean, the thing that, the reason I I, you know, proposed a, a some sort of tru or, or or piece a year ago was because I've predicted pretty much exactly what would would happen, which is a lot of people dying for basically almost no changes in land. And this, the, the, the loss of the, the flower of Ukrainian and Russian youth. And we should have some sympathy for the, the Russian boys as well as the Ukrainian boys. 'cause they, Russian boys didn't, didn't ask to be on their front line. They have to be. So there's a lot of sons not, not coming back to their parents, you know, and, and I think most of 'em don't, don't really have, they don't hate the other side.

2 (18m 45s):
You know, it's sort of like as this saying about, like, this saying comes from World War I, it's like young boys who don't know each other, killing each other on behalf of old men that do know each other. The hell's the point of that.

0 (19m 2s):
So Vladi Zelensky said that he's not, or has said in the past, he's not interested in talking to Putin directly. Do you think he should? Yes. Sit down man to man, lead a leader and negotiate peace?

2 (19m 14s):
Look, I think I would just recommend do not send the flower of Ukrainian youth to be, to die entrenches. Whether he talks to Putin or not. Just don't do that. Whoever goes on the offensive will lose massive numbers of people and history will not look kindly upon them.

0 (19m 42s):
You've spoken honestly about the possibility of war between us and China in the long term. If no diplomatic solution is found, for example, on the question of Taiwan and one China policy, right? How do we avoid the trajectory where these two superpowers clash?

2 (19m 58s):
Well, it's, it's worth reading that book on the, the difficult to pronounce the Acidities trap, I believe it's called. I love war history. I like inside out and backwards. There's hardly a battle I haven't read, read about. And, and trying to figure out like what, what really was the cause of victory in any particular case as opposed to what one side or another claim? The reason

0 (20m 21s):
Both the victory and what sparked the war and Yeah,

2 (20m 24s):
Yeah,

0 (20m 25s):
The whole thing.

2 (20m 27s):
Yeah. So that Athens and Sparta is a classic case. The thing about the Greeks is they really wrote down a lot of stuff. They loved writing, you know, there are lots of interesting things that happened in many parts of the world, but they, people didn't write down. So we don't know what happened, or they didn't really write with in detail. They just would say like, we went, we had a battle and we won. And like, well, what can you add a bit more? The, the Greeks, they really wrote a lot, they were very articulate on, they just love writing. So, and we have a bunch of that writing that's preserved. So we know what led up to the Peloponnesian War between the East Partan and Athenian Alliance.

2 (21m 8s):
And we, we know that they, they, for quite, they they saw it coming. I mean, the Spartans didn't write. They, they also weren't very verbose by their nature, but they did write, but they weren't very verbose. You know, they were tur. But the, the Athenians and the other Greeks wrote, wrote a line and they were like, and Sparta was really kinda like the leader of, of Greece. But, but Athens grew stronger and stronger with each passing year. And, and everyone's like, well that's inevitable that there's gonna be a clash between Athens and s Barta. Well, how do we avoid that? And they couldn't, they couldn't. They actually, they saw it coming and they still could not avoid it.

2 (21m 53s):
So, you know, at some point if there's, if, if one group, one civilization or, or country or whatever exceeds another sort of, like, if, you know, the United States has been the biggest K kid on the block for, since I think around 1890 from an economic standpoint. So the United States has been the economic most powerful economic engine in the world, longer than anyone's been alive. And the foundation of war is economics. So now we have a situation in the case of China where the, the economy is likely to be two, perhaps three times larger than that of the us.

2 (22m 46s):
So imagine you're the biggest kid on the block for as long as anyone can remember. And suddenly a kid comes along who's twice your size.

0 (22m 56s):
So we see it coming. Yeah. How is it possible to stop? Is there some, let me throw something out there, just intermixing of cultures understanding. There does seem to be a giant cultural gap in understanding of each other. And you're an interesting case study because you are an American, obviously. Yes. You've done a lot of incredible manufacturer here in the United States, but you also work with China. I've

2 (23m 20s):
Spent a lot of time in China and met with the leadership many times. Maybe

0 (23m 23s):
A good question to ask is, what are some things about China that people don't understand positive, just in the culture? What's some interesting things that you've learned about the Chinese?

2 (23m 36s):
Well, the, the sheer number of really smart, hardworking people in China is incredible. There are really say like how many smart, hardworking people are there in China. There's far more of them there than there are here. I think it might, in my opinion, the, and they've got a lot of energy. So I mean, the architecture in China that's in recent years is far more impressive than the us. I mean the, the train stations, the buildings, the high speed rail, everything, it's really far more impressive than what we have in the us.

2 (24m 19s):
I, I mean, I recommend somebody just go to Shanghai and Beijing, look at the buildings and go to, you know, take the train from Beijing to Shian, where you have the terracotta warriors. China's got an incredible history, very long history. And you know, I think arguably the, in terms of the use of language from, from a written standpoint, sort of one of, one of the oldest, perhaps perhaps the oldest written language. And, and then China, people did write things down. So now China historically has always been, with rare exception, been internally focused.

2 (24m 59s):
They have not been inquisitive. They've, they've fought each other. They've been many, many civil wars in the three kingdoms war. I believe they lost about 70% of their population. So, and so the, they've had brutal internal wars, like civil wars that make the US civil war look small by comparison. So I think it's important to appreciate that China is not monolithic. We sort of think of like China as this sort of one entity of one mind.

2 (25m 39s):
And this is definitely not the case from what I've seen. And I think most people who understand China would agree. People in China think about China 10 times more than they think about anything outside of China. So it's like 90% of their consideration is, you know, or is, is is internal.

0 (26m 1s):
Well, isn't that a really positive thing when you're talking about the collaboration and the future piece between superpowers when you're inward facing, which is like focusing on improving yourself versus focusing on Yeah. Quote unquote improving others through military might

2 (26m 18s):
The good news, the history of China suggests that China is not inquisitive. Meaning they're not gonna go out and invade a whole bunch of countries. Now they do feel very strongly, you know, so that's, that's good. I mean, 'cause a lot of, a lot of very powerful countries have been inquisitive. The US is one of the, also one of the rare cases that has not been inquisitive. Like a, after World War ii, the US could have basically taken over the world and any country, like we got nukes, nobody else got nukes. We don't even have to lose soldiers. Which country do you want? And the United States could have taken over everything. Oh wait, Atwell. And it didn't, and the United States actually helped rebuild countries. So it helped rebuild Europe, you know, it helped rebuild Japan.

2 (27m 2s):
This is very unusual behavior, almost unprecedented. You know, the US did conspicuous acts of kindness like the Berlin airlift, you know, and, and I think, you know, it's, it's always like, well, America's done bad things. Well, of course America's done bad things. But one needs to look at the, the whole track record and, and just generally, you know, one, one sort of test would be how do you treat your prisoners at war? Or let's say, you know, no offense to the Russians, but let's say you're in Germany, it's 1945, you've got the Russian army coming on one side, and you've got the French, British and American armies coming the other side.

2 (27m 53s):
Who would you like to be to surrender it to? Like, no country is like Marly perfect, but I recommend being APOW with the Americans. That would be my choice very strongly

4 (28m 7s):
In the full menu of POWs

2 (28m 9s):
In the us Very much so. And in fact, Von Brown, yeah. Took, you know, smart guy was like, we've got to be captured by the Americans. And in, in fact, the SS was under orders to execute Bo Brown and all of the German rock conditioners. And they narrowly escaped their ss. They, they said they were going out for a walk in the woods. They left in the middle of winter with no coats, and they ran like no food, no coats, no water, and just ran like hell and ran West. And by Sherlock, they, I think his brother found like a, a bicycle or something and, and then just cycled west as fast as he couldn't have found, found a US patrol.

2 (28m 59s):
So anyway, that's, that's one, that's one way you can tell morality is who, who, where do you wanna be APW It's not fun anywhere, but some places are much worse than others. So anyway, so, so, so like America has been, while far from perfect, generally a, a benevolent force, and we should always be self-critical and we try to be better, but anyone with half right knows that. So, so I think there are some in this way, China and the United States are similar. Ne neither country has been acquisitive in, in a significant way.

2 (29m 43s):
So that's like a, you know, a a shared principle I guess. Now, now China does feel very strongly about Taiwan. They've been very clear about that for a long time. You know, from their standpoint, it's, it's, it would be like one of the states is, is is, you know, not there like, like Hawaii or something like that. But, but more significant than Hawaii, you know, and Hawaii is pretty significant for us. So they, they view it as, as, as really the, that there's a fundamental part of China, the islander Formosa now, now Taiwan, that is not part of China, but should be.

2 (30m 28s):
And the only reason it, it hasn't been is 'cause the US Pacific fleet

0 (30m 32s):
And is their economic power grows and is their military power grows. The thing that they're clearly saying is their interest will, you know, clearly be materialized.

2 (30m 47s):
Yes. At China has been very clear that they'll incorporate Taiwan peacefully or militarily, but that they will incorporate it from their standpoint is 100% likely.

0 (31m 4s):
You know, something you said about conspicuous acts of kindness as a geopolitical policy. It almost seems naive, but I'd venture to say that this is probably the path forward, how you avoid most wars. Just as you say it, it sounds naive, but it's kind of brilliant. If you believe in the goodness of underlying most of human nature, it just seems like conspicuous acts of kindness can reverberate through the populace of the countries involved.

2 (31m 41s):
Yeah, well

0 (31m 43s):
And deescalate,

2 (31m 44s):
Absolutely. So in, in after World War I, the, the, they made a big mistake. You know, they, they basically tried to lump all of blame on Germany and, and, and, you know, saddle Joan with impossible reparations. And you know, really, there was a lot of, there was a fair, quite a bit of blame to go around for World War I, but they, they try to, you know, put it all in Germany and that was that, that laid the seeds for World War ii.

2 (32m 25s):
So that's a lot of people were not just Hitler, A lot of people felt wronged and they wanted vengeance and they got it.

0 (32m 38s):
People don't forget.

2 (32m 40s):
Yeah, you, you, you know, you kill somebody's father or mother or son, daughter, they're not gonna forget it. They'll want vengeance. So after World War ii, they're like, well, the treat of Versailles was a huge mistake in World War I. And so this time, instead of, you know, crushing the losers, we're, we're actually gonna help 'em with the Marshall Plan and we're gonna help Revol Reil Germany, we're gonna help re rebuild or, you know, Austria and the, the other, you know, Italy and whatnot.

2 (33m 20s):
So, and that was the right move.

0 (33m 27s):
There's, it does feel like there's a profound truth to conspicuous acts of kindness being an antidote to this.

2 (33m 37s):
Something must stop the, the cycle of reciprocal violence. Something must stop it or it will, you know, it'll, it'll, it'll never stop. Just eye for an eye tooth, for a tooth limb, for a limb life, for a life for forever and ever

0 (33m 57s):
To escape. Briefly, the darkness was some incredible engineering work. xAI just released Grok AI assistant that I've gotten a chance to play with. It's, it's amazing on many levels. First of all, it's amazing that a relatively small team in a relatively short amount of time was able to develop this closest state-of-the-art system. Another incredible thing is there's a regular moat and there's a fun mode.

2 (34m 23s):
Yeah, I guess I'm to blame for that one.

0 (34m 27s):
I wish, first of all, I wish everything in life had a fun mode. Yeah. I, there there's something compelling beyond just fun about the fun mode. Yeah. Interacting with a large language model. I'm not sure exactly what it is 'cause I've only have had a little bit of time to play with it, but it just makes it more interesting, more vibrant to interact with the system. Yeah,

2 (34m 50s):
Absolutely. I, our, our, our AI Grok is modeled after the Hitchhike has Got to the Galaxy, which is one of my favorite books, which is, it's a book on philosophy, disguises book on humor. And I would say that is, that forms the basis of my philosophy, which is that we don't know the meaning of life, but the more we can expand the scope and scale of consciousness, digital and biological, the more we're able to understand what questions to ask about the answer that is the universe.

2 (35m 30s):
So I have a philosophy of curiosity.

0 (35m 34s):
There is generally a feeling like this AI system has an outward looking like the way you are, like sitting with a good friend looking up at the stars, like the asking pod head, like questions about the universe, wondering what it's all about. The curiosity you talk about there, there's a sense no matter how long day in the question I ask it, there's, there's a sense of cosmic grandeur to the whole thing. Well

2 (35m 59s):
We, we are actually working hard to have engineering math Physics answers that you can count on. So for the other sort of AI out there that these so-called large language models, I've not found the engineering to be reliable and it, it, the hallucination, it, it unfortunately hallucinates mo most when you at least want it to hallucinate. Yeah. So when you ask important diff difficult questions, that that's when it tends to be confidently wrong. So we're really trying hard to say, okay, how do we be as grounded as possible?

2 (36m 42s):
So you can count on the results, trace things back to Physics first principles, mathematical logic. So underlying the humor is an aspiration to ad adhere to the truth of the universe as closely as possible.

0 (37m 1s):
That's really tricky.

2 (37m 3s):
It is tricky. So that's why, you know, you, there's always gonna be some amount of error. But do we want to aspire to be as truthful as possible about the answers with acknowledged error? So that there was always, you don't wanna be confidently wrong, so you're not gonna be right every time, but you don't be, you wanna minimize how often you're confidently wrong. And then, like I said, once you can count on the logic as being not violating Physics, then you can start to, to build on that, to create inventions like invent new technologies.

2 (37m 46s):
But if, if you can't, if if you're, if you cannot count on the foundational Physics being correct, obviously the inventions are simply wishful thinking, you know, imagination land magic basically.

0 (38m 1s):
Well, as you said, I think one of the big goals of XAI is to understand the universe.

2 (38m 6s):
Yes. That's how simple three word mission.

0 (38m 13s):
If you look out far into the future, do you think on this level of Physics, the very edge of what we understand about Physics, do you think it will make discoveries, sort of the sexiest discovery of them as, as we know now, sort of unifying general relativity and quantum mechanics. So coming up with a theory of everything, do you think it could push towards that direction? Almost like theoretical Physics discoveries?

2 (38m 38s):
If an AI cannot figure out new Physics, it's clearly not equal to humans, let alone nor nor has sur surpassed humans. 'cause humans have figured out new Physics, they're just, you know, Physics is just understanding, you know, deepening once inside into how reality works. And then, then, then, then there's engineering, which is inventing things that have never existed. Now the, the range of possibilities for engineering is far greater than for Physics. 'cause you know, once you figure out the rules of the universe, that that's, that's it. You've discovered things that already existed. But from that you can then build technologies with that are really almost limitless in the variety and cap.

2 (39m 22s):
You know, it's like once you understand the rules of the game properly, and we do, you know, with current Physics, we do at least at a local level, understand how Physics works very well. Where our ability to predict things is incredibly good. Like quantum mechanics is the degree to which quantum mechanics can predict outcomes is incredible. That was my, that's my hard, hardest class in college, by the way. My, my, my senior quantum mechanics class was harder than all of my other classes put together

0 (39m 51s):
To get an AI system, a large language model to, to reliably be as reliable as quantum mechanics and Physics is very difficult.

2 (40m 1s):
Yeah. You have to test any, any conclusions against the ground. Truth of reality. Reality is the ultimate judge. Like Physics is the law. Everything else is a recommendation. I've seen plenty of people break the, I break the laws made by man, but none break the laws made by Physics.

0 (40m 16s):
Yeah, it's a good test actually. If this LLM understands and matches Physics, then you can more reliably trust whatever it thinks about the current state of politics in some sense.

2 (40m 28s):
And it's, it's also not, not the case currently that even that its internal logic is not consistent. So especially with these, with the approach of like just predicting a token predict token predict token, it's like a vector sum. You know, you, you, you're summing up a bunch of vectors, but you, you can get drift. So as those, a little bit of error, a little bit of error adds up. And by the time you are many tokens down the path, you're, it, it doesn't make any sense.

0 (40m 60s):
So it has to be somehow self-aware about the drift.

2 (41m 2s):
It has to be self-aware about the drift, and then look at the thing as a gestalt as a whole and, and say it does it have coherence as a whole. So, you know, when, when authors write books that they, they will write the book and then they'll go and revise it, you know, taking into account, you know, all the, the end in the beginning and the middle and, and rewrite it to achieve coherence so that it doesn't end, end up in a nonsensical place.

0 (41m 34s):
Maybe the process of revising is what

2 (41m 36s):
Yeah.

0 (41m 37s):
Reasoning is, and then that's the process of revising is how you get closer and closer to truth. Maybe you like, I at least I approached it that way. You just say a bunch of bullshit first and then you get it better. You start a bullshit. Yeah. And then you get, you

2 (41m 49s):
Create a draft and then, and then you, and then you iterate on that draft until it has has coherence until it's, it all adds up basically.

0 (41m 59s):
So another question about theory of everything, but for intelligence, do you think there exists as you're exploring this with XAI, creating this intelligence system? Do you think there is a theory of intelligence where you get to understand what, like what is the I in AGI and what is the I in human intelligence?

2 (42m 22s):
There's no I in Team America? Oh, wait, there is

0 (42m 27s):
No, it's gonna be stuck in my head. No. Yeah, there's, there's no me and whatever in quantum mechanics. Oh, wait, I mean is that part of the process of discovering, understanding the universe is understanding intelligence?

2 (42m 48s):
Yeah. Yeah. I think we need to understand intelligence, understand consciousness. I mean there, there, I mean there are some sort of fundamental questions of like, what is thought? What is emotion? Yeah. Is it really just one atom bumping into another atom? It feels like something more than that. So I-I-I-I-I think we're probably missing some really big things. Like some really big things.

0 (43m 18s):
Something that'll be obvious in retrospect. Yes. Like there's a giant, like you, you put the whole consciousness and motion.

2 (43m 27s):
Well some people would quote like a, like a soul, you know, in religion. Yeah. A soul be a soul. Like you feel like you're you, right? I mean, you don't feel like you're just a collection of Atoms, but on what dimension does thought exist? What dimension does do emotions exist? We feel them very strongly. I suspect there's more to it than Atoms bumping into Atoms.

0 (43m 52s):
And maybe AI can pave the path to the discovery of what whatever the hell that thing is.

2 (43m 58s):
Yeah. What is consciousness like? What are, when you put the Atoms in a particular shape, why are they able to form thoughts and take actions that, that and, and feelings.

0 (44m 10s):
And even if it is an illusion, why is this illusion so compelling? Yeah. Like

2 (44m 15s):
How do, why does this illusion exist?

0 (44m 17s):
Yeah.

2 (44m 18s):
On what plane does this, this illusion exist?

0 (44m 22s):
Yeah.

2 (44m 23s):
And that sometimes I wonder is, you know, either perhaps everything's conscious or nothing is conscious one of the two?

0 (44m 33s):
I like the former, everything conscious just seems more fun.

2 (44m 38s):
It does seem more something more fun. Yes. But we're we're composed of Atoms and those Atoms are composed of quarks and leptons. And those quarks and leptons have been around since the beginning of the universe.

0 (44m 50s):
The beginning of the universe.

2 (44m 53s):
Right? What, what seems to be the beginning of the universe,

0 (44m 56s):
The first time we talked, you said what you would, which is surreal to think that this discussion was happening is becoming a reality. I asked you what question would you ask an AGI system once you create it? And you said, what's outside the simulation is the question. And good question. Yeah. But it seems like with Grok you started to literally, the system's goal is to be able to ask such questions, to answer such questions and to ask such questions.

2 (45m 24s):
Where are the Aliens? Where

0 (45m 25s):
Are the Aliens?

2 (45m 26s):
That's one of the, the, like, the paradox question. A lot of people have asked me if, if I've seen any evidence of Aliens and I've, I haven't, which is kind of concerning. 'cause then I think what I'd probably prefer it at least to have seen some archeological evidence of Aliens, to the best of my knowledge, there is no proof. I, I'm not aware of any evidence surveillance. If they're out there, they're very subtle. We might just be the only consciousness, at least in the galaxy. And if you, if you look at say the history of Earth four, four to believe the archeological record Earth is about four and a half billion years old.

2 (46m 6s):
Civilization as measured from the first writing is only about 5,000 years old. We have to give some credit there to the ancient Sumerians who aren't around anymore. I think it was a archaic pre-form was the first actual symbolic representation, but only about 5,000 years ago. I think that's a good date for when we say civilization started. That's 1000000th of Earth's existence. So civilization has been around, it's really a flash in the pan so far. And why, why have we, why did it take so long for, you know, four and a half billion years?

2 (46m 57s):
For the vast majority of the time, there was no life. And, and then there was archaic there bacteria for a very long time. And then, you know, you had mitochondria get captured, multicellular life differentiation into plants and animals. Life moving from the oceans to land mammals, higher brain functions. And the sun is expanding slowly, but it, it will, it'll overheat, it'll, it'll heat, heat the earth up at some point in the future, boil the oceans and and earth will become like Venus.

2 (47m 42s):
Where, where near life, life as we know it is impossible. So if we do not become multi-planetary and ultimately coa solar system, annihilation of all life on earth is a certainty. A certainty. And it could be as little as on the galactic timescale, half a billion years, you know, long time by human standards. But that, that's only 10% longer than earth has been around at all. So if, if, if life had taken 10% longer to evolve on earth, it wouldn't exist at all.

0 (48m 27s):
We got a deadline coming up. Yeah. But a hurry. But that said, as you said, humans intelligent life on earth developed a lot of cool stuff very quickly. So yes. It, it seems like becoming a multi-planetary is almost inevitable. Unless we destroy this

2 (48m 46s):
Thing, we need to do it. I mean, it's, it's not, I mean, I, I suspect that there, there, if we are able to go out there and explore other star, star systems that we, there's a good chance we find a whole bunch of long dead one planet civilizations. Yeah. Then have made it past their home planet.

0 (49m 3s):
That's so sad. It's sad. Also fascinating.

2 (49m 8s):
I mean there are various explanations for the, for paradox. And one is, is the sort of, they're these great filters which civilizations don't pass through. And one of those great filters is do you become a multi-plan civilization or not? And if you don't, it's simply a matter of time before something happens on your planet. You know, either natural or manmade that causes us to die out. Like the dinosaurs, where are they now? They didn't have spaceships. So

0 (49m 42s):
I think the more likely thing is 'cause just to empathize with the Aliens that they, they found us and they're protecting us and letting us be,

2 (49m 51s):
I hope so would nice Aliens

0 (49m 53s):
Just like the tribes in the, in the Amazon, the uncontacted tribes will protecting them. That's what,

2 (49m 59s):
That would be a nice explanation. Or

0 (50m 1s):
You could have like a, what was it? I think Andre Kathe said, it's like the ants and the Amazon asking where's everybody?

2 (50m 10s):
Well, they do run into a lot of other ants.

0 (50m 13s):
That's true.

2 (50m 14s):
They have these ant wars.

0 (50m 16s):
Sounds like a good TV show. Yeah.

2 (50m 18s):
They literally have these big wars between various ants.

0 (50m 22s):
Yeah. I'm, I'm, maybe I'm just dismissing all the different diversity of, you should

2 (50m 28s):
Listen to that Warner Herzog talking about the jungle. It's really hilarious. Have you heard it?

0 (50m 32s):
No, I have not. It's awesome. But Warner Herzog is a way

2 (50m 37s):
You should play, you should play it for the, you know, as an interlude in the Yeah, it's on YouTube. It's, it's awesome.

0 (50m 45s):
I love him so much. Yeah, he's great. Was he the director of Happy People Life and the Taiga? I think also he

2 (50m 51s):
Did that bear documentary, the Bear

0 (50m 53s):
Documentary.

2 (50m 53s):
Yeah. I did this thing about penguins. Yeah.

0 (50m 57s):
The dere the analysis, the psychoanalysis of

2 (50m 59s):
Penguins psycho. Yeah. The penguins like headed for like the mountains like that are like 70 miles away. Yeah. And Penguin is just headed fordum basically.

0 (51m 9s):
Well, he was had a cynical take. I I have a, he could be just a Brave explorer and, and there'll be great stories told about him amongst the penguin population for many centuries to come. What were we talking about? Okay.

2 (51m 28s):
Yeah. So Aliens, I mean, I, I don't know. Look, I think it, the smart move is just, you know, this is the first time in the history of earth that it's been possible for life to ex ex extend beyond earth. That window is open now, it may be open for a long time or it may be open for a short time and it, it may be open now and then never open again. So I, I think the smart move here is to make life multiplanetary while it's possible to do so. We don't wanna be one of those lame one planet civilizations. No. That just dies

0 (52m 4s):
Out. No, those are lame. Yeah. Lame self-respecting

2 (52m 10s):
Civilization would be one planet.

0 (52m 11s):
There's not gonna be a Wikipedia entry for one of one of those. And pause the SpaceX have an official policy for when we meet Aliens.

2 (52m 24s):
No. Okay.

0 (52m 25s):
That seems irresponsible.

2 (52m 31s):
I mean, look, if if, if I see the slightest indication that there are Aliens, I will immediately post on X platform. Yeah. Anything I know

0 (52m 39s):
It could be the most liked reposted post of all time.

2 (52m 42s):
Yeah. I mean look, we, we have more satellites up there right now than everyone else combined. So, you know, we'd know, we know if we've got a maneuver around something and we not don't have to maneuver around anything.

0 (52m 55s):
If we go to the big questions once again, you said you've, you're with Einstein that you believe in the God of Spinoza.

2 (53m 4s):
Yes.

0 (53m 5s):
So you know that that's a view that God is like the universe and is reveals himself through the laws of Physics or as Einstein said, through the lawful harmony of the world.

2 (53m 16s):
Yeah, I would agree that, that God of the, the simulator or whatever the, the supreme being or beings reveal themselves through the Physics, you know, they have creatives of this existence and it's incumbent upon us to try to understand more about this one risk creation.

0 (53m 38s):
Like who created this thing, who's running this thing? Like embodying it into a singular question with a sexy word on top of it is like focusing the mind to understand it. It does seem like there's a, again, it could be an illusion. It seems like there's a purpose that there's an underlying master plan of some kind. And it seems like

2 (53m 59s):
There may not be a master plan in the sense. So there's like, maybe an interesting answer to the question of determinism versus free will is that if we are in a simulation, the reason that the, the, these higher beings would hold a simulation is to see what happens. So it's not, they don't know what happens otherwise. They wouldn't hold the simulation. So when, when humans create a simulation, so it's SpaceX and Tesla, we create simulations all the time, especially for the rocket. You, you, you know, you have to run a lot of simulations to understand what's gonna happen because you can't really test the rocket until it goes to space and you want it to work.

2 (54m 45s):
So you have to, you have to simulate subsonic, transonic, hyper supersonic, hypersonic ascent, and then coming back super high heating and orbital dynamics. All this is gonna be simulated. So, 'cause you don't get very many kicks at the can. But we, we run the simulations to see what happens. Not if we knew what happens, we wouldn't run the simulation. So if, if there's, so whoever created this existence is they're running it because they don't know what's gonna happen. Not because they do.

0 (55m 24s):
So maybe we both played Diablo, maybe Diablo was created to see if it druid your character could defeat Uber Lilith at the end. They didn't know.

2 (55m 34s):
Well, the funny thing is u her title is Hatred Incarnate.

0 (55m 41s):
Yeah.

2 (55m 43s):
And right now, I guess you can ask the Diablo team, but it's almost impossible to defeat hatred in the eternal realm.

0 (55m 55s):
Yeah. You've streamed yourself dominating tier 100 nightmare dungeons. Yeah, I can. And still,

2 (56m 1s):
I, I, I can cruise through tier 100 nightmare dungeons like a stroll in the puck.

0 (56m 7s):
And still you're defeated by hatred.

2 (56m 9s):
Yeah, I can, there's the, the sort of, I guess maybe the second hardest boss is durel durel can't even scratch the paint. So I killed Darel Durel so many times and every other boss in the game, all, all of them kill 'em So many times it's easy. But Uber Lilith, otherwise known as hatred incarnate, especially if you're a dru and you have no ability to go and to be vulnerable, you, there are these random death waves that, that come at you. And I'm pretty, you know, I really am 52, so my reflex is not what they used to be, but I'm, I have a lifetime of playing Video Games.

2 (56m 51s):
At one point I was, you know, maybe one of the best quake players in the world actually won money for in, in what I think was the first paid eSports tournament in the US we're doing, doing four person quake tournaments. And we came second, I was the second best person on the team and the, the, the actual best person that we we're actually winning, we were gonna come first except the best person on the team. His computer crashed for the game. So we, we came second, but I got money for it and everything. So like, basically I got skills, you know, albeit, you know, no, no spring, spring chicken these days. And the, to be totally frank, it's driving me crazy trying to beat Lilith as a dr basically try to, trying to beat, trying to beat hatred and con in the eternal realm

0 (57m 41s):
As a drut. As

2 (57m 42s):
A druitt. And if you, if you, if you, this is really ing lemme tell you,

0 (57m 49s):
I mean, the challenge is part of the fun. I I have seen directly like you're actually like a world class incredible video game player. Yeah. And I think Diablo, so you're just picking up a new game and you're figuring out it's fundamentals. You're also with the Paragon board and, and the build are not somebody like me who perfectly follows whatever they suggest on the internet. You're also an innovator there. Yeah. Which is hilarious to watch. It's like a, it's like a mad scientist just trying to figure out the Paragon board and, and, and the build and the Yeah. You know, is there some interesting insights there about if, if somebody's starting as a druid, do you have advice?

2 (58m 31s):
I would not recommend playing a druid in the eternal realm. No. Right now, I think the most powerful character in this, in the seasonal realm is the Sora with the lightning balls. The, the soss have huge balls in the seasonal,

0 (58m 46s):
Well yeah, that's what they say.

2 (58m 49s):
It's soss have huge balls. They do huge bulls of lightning.

0 (58m 54s):
Right. I'll take you a word for it.

2 (58m 56s):
And it's actually in, in the seasonal realm, you can, you can, it's, it's like pretty easy to beat Uber Lo with the va VA because you get these vapor powers that out fire damage and increase your defense and whatnot. So really quite easy to defeat hatred seasonally, but to defeat hatred eternally very difficult, almost impossible. It's very, I impossible, it, it seems like this's a metaphor for life, you know? Yeah.

0 (59m 25s):
I like the idea that Elon Musk because I saw, I was playing Dali yesterday and I saw 100 level, 100 drew just run by, I will never die and then run back the other way. Yeah. And it was, there's just some, this metaphors kind of hilarious that you, Elon Musk is fighting hatred, restlessly fighting hatred in this demonic realm. Yes.

2 (59m 48s):
It's,

0 (59m 48s):
It's hilarious. I mean, it's pretty hilarious. No,

2 (59m 50s):
It's absurd. Really. It's exercise and absurdity and it makes me wanna pull my hair out. Yeah.

0 (59m 58s):
I What do you get from Video Games in general? Is there, is there for you, for you personally?

2 (1h 0m 3s):
I mean, it's, it's, I don't know if it's, it calms my mind. I mean, you sort of killing the demons in a video game calms the demons in my mind.

0 (1h 0m 14s):
Yeah.

2 (1h 0m 15s):
If, if you play a tough video game, you can get into like a state of flow, which is very enjoyable. And, but the admittedly it, it needs to be not too easy, not too hard, kinda in the Goldilocks zone. And I guess you generally wanna feel like you're progressing in the game. So a good video and, and there's also beautiful art, engaging storylines and it's a, it's, it's like an amazing puzzle to solve, I think. And so it's like solving the puzzle.

0 (1h 0m 52s):
El Ring the greatest game of all time. I still haven't played it, but you,

2 (1h 0m 56s):
It's Al Alden Ring is definitely a candidate for best game ever top five for sure.

0 (1h 1m 1s):
I think I've been scared how hard it is or how hard I hear it is. So, but it is beautiful.

2 (1h 1m 7s):
Alden ring is, feels like it's designed by an alien.

0 (1h 1m 12s):
Hmm.

2 (1h 1m 13s):
There's

0 (1h 1m 14s):
A theme to this discussion. In what way?

2 (1h 1m 16s):
It's, it's, it's, it's so unusual. It's incredibly creative and the art is stunning. I recommend playing it on a, on a big resolution pie dynamic. Raise TV even doesn't need to be a monitor, just a, the art is incredible. It's so beautiful and, and it's, it's so unusual. And each of those top bus battles is unique. Like, it's like a unique puzzle to solve each one's different. And the, the strategy you use to solve one battle is d different from another battle.

0 (1h 1m 54s):
That said, you said drew it an internal against Uber. Lilith is the hardest boss battle you've ever

2 (1h 1m 60s):
Correct. That is currently the, the, and I've, I've played a lot of Video Games my, because mys my primary rec recreational activity. Yes. And yes, beating hatred in the internal realm is the hardest bus battle in life. And in the video game metaphor and top metaphor, like, I don't, I don't, I'm not sure it's possible, but it's, it, it's, I do make progress. So then I'm like, okay, I'm making progress. Maybe if I just tweak that paragon board a little more, I can do it could just dodge a few more waves.

2 (1h 2m 42s):
I can do it.

0 (1h 2m 43s):
Well the simulation is created for the purpose of figuring out if it can be done. And you are just a cog in that simulation, in the machine of the simulation. Yeah.

2 (1h 2m 52s):
It might be. I-I-I-I-I have a feeling that at least I, I think it's doable. It's, it's doable. Yes.

0 (1h 3m 5s):
Well that's the human spirit right there to believe.

2 (1h 3m 9s):
Yeah. I mean, did prompt me to think about just hate in general, which is you, you, you want, you wanna be careful of the one of those things where you wish for something that sounds good, but in, but if you get it, it's actually a Dystopian situation. So, you know, you could sort of run a sort of have hot sort of like if you wish for world peace sounds good, but how is it enforced? And, and at what cost is, is at what cost? Eternal peace. It might actually be worse to have eternal peace because of what that would entail.

2 (1h 3m 50s):
The suppression of everyone. It might be the suppression of progress. It might be an ossified society that never changes. What I'm saying is that there is an argument that you, you, that if that if you wish for no war, you should be careful what you wish wishful. Because what's required in order for there to be no war might be worse than a little war.

0 (1h 4m 22s):
Yeah. The things we'll consider to be flaws of human civilization might be a necessary components for whatever optimal looks like. I mean this, do you worry about ai, AGI enabling a Dystopian state of this nature, whether it's 1984 with surveillance and fear or Brave, new world with pleasure and what is it? Lot lots of sex, but no deep human experience.

2 (1h 4m 51s):
You know, there, there's actually a real drug called Soma. There is, yes. I'm like, take notes. Did they actually, does GR know about

0 (1h 4m 59s):
It? No, it probably will go to Brave new world.

2 (1h 5m 2s):
We could ask it.

0 (1h 5m 3s):
What? What would be the question?

2 (1h 5m 6s):
Tell us about the drug called Soma.

5 (1h 5m 8s):
Tell us about the drug called Soma.

0 (1h 5m 13s):
It's gonna go to Brave New world I think.

2 (1h 5m 15s):
Well there's a real drug as well

0 (1h 5m 18s):
Searching for Soma. Drug information. Oh boy. Ah, Soma. The wonder drug by the way. I'm in fun mode.

2 (1h 5m 28s):
Oh, okay. Fun. Yeah. So it's talking about the real drug.

0 (1h 5m 31s):
Oh, it's a muscle relaxant that's been around since the 1950s. It's used to treat muscle pain and discomfort. And it does this by altering the activity between neurons and the spinal cord. Now Soma isn't just a pain reliever, it's also a bit of a sedative. It can make you feel a bit drowsy. So it's not something to take if you're planning on operating, having machinery or trying to solve complex math equations. And it goes on and on. In conclusion, soma's a drug that can be helpful for those dealing with muscle pain, but it's not without its risks. It's pretty good. Oh wait a minute. If that's the

2 (1h 6m 2s):
Real drug it's talking about,

0 (1h 6m 3s):
It's the real drug. Yeah. And of course the fun is always use it responsibly in accordance with the advice of a healthcare professional. And remember in the end, the universe is probably just a giant computer simulation. So don't take anything too seriously. I love it. I love it. I love it. I love it.

2 (1h 6m 21s):
Yeah. So,

0 (1h 6m 22s):
Oh, and also really nicely includes like the, the tweets posts on X that are relevant to this, which is really cool. Yeah. It like integrates everything together. Yeah. Like the interface is one of the really cool things here. Yeah.

2 (1h 6m 37s):
Seems like a great job. So, and you say like, you could say like, no, no, I mean, Soma from brain Brave New World because it, it thinks you mean, you, you mean Soma the real drug? Yeah. No, I mean, Soma from brain beer world,

0 (1h 6m 57s):
It's gonna be like, oops, I'm really sorry. I'm really surprised it went to the real drug first. Ah, my apologies. I was thinking of the muscle relax. And Soma for brand new world is a different beast altogether. No, this Huxley Dystopian now. Soma is a drug that plays a central role in the society, depicted in the book. It's a mood altering hallucinogenic drug that's used to keep the population docile and content in the novel to use Soma and Brave. New World raises some interesting questions about the nature of happiness and the role of drugs in society. Oh, man. Exactly.

2 (1h 7m 31s):
Is it better to live in a world where everyone is happy all the time, even if that happiness, happiness is artificial? Good question. This is what I mean. Like, do you want, do you wish for world peace and, and happiness all the time? Are you sure? Because that might be a society that is essentially sterile and ossified that never changes, that is ultimately doomed

0 (1h 7m 58s):
This kind of tension between

2 (1h 8m 2s):
Doctors and the light. It's, it's, it's, this is really a very good summary. It really gets to the point. This is not simply regurgitating a Brave new world. It's actually getting to the salient element of Soma as a drug. Do you, do you actually want to be in a situation where everyone is happy all the time, even though it's artificial? Or is it better to confront the challenges of life and experience the full range of human emotions, even if it means experiencing pain and suffering

0 (1h 8m 31s):
For those listening? By the way, Ilan just read directly from Grok, which is a really nice, kind of insightful, philosophical analysis of the tension here. Interesting.

2 (1h 8m 41s):
It pretty much nails it. In conclusion, Selma from Brave New World is fictional drug that's used to explore some deep philosophical questions about the nature of happiness and the role of drugs in society. It's a powerful symbol of the dangers of using drugs to escape from reality and the importance of confronting the challenges of life head on. Nailed it. And the crazy thing is like, you know, we do have a real drug called Soma, which kind of is, is kind of like the drug in the book. And I'm like, did they, they must've named, they must've named it after. Yeah,

0 (1h 9m 13s):
Probably, probably.

2 (1h 9m 14s):
Yeah. Some of the, the real drug is quite effective on back pain.

0 (1h 9m 18s):
So, you know about this drug. I've taken it, it's fascinating. Okay.

2 (1h 9m 21s):
Because I had like a, you know, squashed disc in my C five C six.

0 (1h 9m 26s):
So it takes the physical pain away. But Soma here is, it doesn't

2 (1h 9m 29s):
Completely, it, it, it reduces the amount of pain you feel, but at the expense of mental acuity, it dells your mind. Just like, just like the drug in the book. Just

0 (1h 9m 42s):
Like the truck in the book. Yeah. And hence the trade off. The thing that seems like utopia could be a dystopia after all.

2 (1h 9m 49s):
Yeah. And actually I was talking to a friend of mine saying like, would you really want there to be no hate in the world? Like, really none. Like, I wonder why hate evolved. I'm not saying we should amplify hate. Of course we should try to minimize it, but none at all. Hmm. There might be a reason for hate

0 (1h 10m 13s):
And suffering. I mean, it's really complicated to consider that some amount of human suffering is necessary for human flourishing.

2 (1h 10m 22s):
Is it possible to appreciate the highs without knowing the lows?

0 (1h 10m 29s):
And that, that, that all is summarized there in a single statement from Crock. Okay.

2 (1h 10m 35s):
No highs, no lows, who knows?

0 (1h 10m 38s):
That's almost the poem. It seems that training LMS efficiently is a big focus for XAI. What's the, first of all, what's the limit of what's possible in terms of efficiency? There's this terminology of useful productivity per watt. Like what have you learned from Yeah. Pushing the limits of that?

2 (1h 10m 59s):
Well, I, I think it's helpful. The, the tools of Physics are very powerful and can be applied, I think to almost any, really any arena in life. There's, it's really just critical thinking for something important. You need to reason with first principles and thinking about things in the limit one direction or the other. So in the limit, even at the kaev scale, meaning even if you harness the entire power of the sun, you'll still care about useful compute for a watch. So that's where I, I think probably where things are headed from the standpoint of AI is that we, we have a silicon shortage now that will transition to a voltage transformer shortage in about a year.

2 (1h 11m 46s):
Ironically, transformers, for transformers, you need, you need transformers to run transformers.

0 (1h 11m 53s):
Somebody has a sense of humor in this thing.

2 (1h 11m 57s):
I think. Yes, man, fate loves irony, ironic humor and, and ironically funny outcome seems to be often what fate wants.

0 (1h 12m 9s):
Humor is all you need. I think spice is all you need. Somebody posted.

2 (1h 12m 14s):
Yeah. But yeah, so, so we're, we're, we're, I have silicon shortage today, a voltage step down transformer shortage probably in about a year. And then just electricity shortages in general. In about two years. I, I gave a speech for the sort of world gathering of utility companies, electricity companies, and I, I said, look, you really need to prepare for a traveling of electricity demand, because all transport is gonna go electric with the ironic exception of rockets. And, and, and heating will also go electric. So energy usage right now is roughly one third, very rough terms.

2 (1h 12m 54s):
One third electricity, one third transport, one third heating. And so in order for everything to go sustainable, to go electric, you need to trouble electricity output. So I encourage the utilities to build more power of plants and, and also to probably have, well, well, not probably, they should definitely buy more batteries because the, the grid currently is size for realtime load, which is kind of crazy. 'cause you know, that means you got a size for whatever the, the peak electricity demand is like the worst second or the worst day of the year. Or you can have a brown out or a blackout, and you're at that, at that crazy blackout for several days in, in, in Austin.

2 (1h 13m 42s):
So because there's almost no buffering of energy in the grid, like if you've got a hydro power plant, you can buffer energy, but otherwise it's all real time. So with batteries, you can, you can produce energy at night and, and use it during the day so you can buffer. So I, I, I expect that there will be very heavy usage of, of batteries in the, because the, the, the peak to trough ratio for power plants is anywhere from two to five, you know, so it's like lowest point to highest point.

0 (1h 14m 20s):
So like batteries necessary to balance it out. And then, but the demand, as you're saying, is going to grow, grow, grow, grow. Yeah. And part of that is the compute?

2 (1h 14m 29s):
Yes. Yes. I mean, electrification, I mean, electrification of transport and, and electric heating will, will be much bigger than ai, at least in the short term. In the short term. But, but even for, for ai, you, you really have a growing demand for electricity, for electric vehicles and a growing demand for electricity, for to run the computers for ai. And so this is obviously leading, can lead to a electricity shortage.

0 (1h 14m 58s):
How difficult is the problem of, in this particular case, maximizing the useful productivity per watt for training? You know, nuts like, this seems to be really where the big problem we're facing that needs to be solved is how to use the power efficiently. Like what you've learned so far about applying this Physics first principle reasoning in this domain. How difficult is this problem?

2 (1h 15m 30s):
It will get solved. It's the question of how long it takes to solve it. So at various points, there's a limit, some, some kind of limiting factor to progress. And with regard to ai, I'm saying right now the limiting factor is silicon chips. And that will, we're gonna then have more chips than we can actually plug in and turn on probably in about a year. The, the initial constraint being literally voltage, step down transformers, because you've got power coming in at 300, 300,000 volts and it's gotta step all the way down eventually to around 0.7 volts. So it's a very big amount of, you know, the voltage step down is gigantic.

2 (1h 16m 17s):
So, and, and the, the industry is not used to rapid growth.

0 (1h 16m 23s):
Okay. Let's talk about the competition here. You've shown concern about Google and Microsoft with open AI developing AGI, how can you help ensure with xAI and Tesla AI work that it doesn't become a competitive race to AGI, but it's, that is a collaborative development of safe AGI,

2 (1h 16m 42s):
Well, I, I mean, I've been pushing for some kind of regulatory oversight for a long time. I've been somewhat of a Cassandra on the subject for over a decade. I think we want to be very careful in how we develop ai. It's a, it's a, it's a great power, and with great power comes great responsibility. I think it, it would be wise for us to have at least an objective third party who can be like a referee that can go in and understand what the various leading players are doing with ai. And even if there's no enforcement ability, they should, they can at least voice concerns publicly.

2 (1h 17m 32s):
You know, ge, Jeff Hinton, for example, left Google and he voiced strong concerns, but now he's not at Google anymore, so who's gonna voice the concerns? So I think, I think there's, I I, I like, I know Tesla gets a lot of regulatory oversight on the automotive front. I mean, we're subject to I think over a hundred regulatory agencies domestically and internationally. So it's, it's a lot. You could fill this room with the all regulations that Tesla has to adhere to for automotive. Same is true in, you know, for rockets and for, you know, currently the limiting factor for SpaceX, for Starship launch is regulatory approval.

2 (1h 18m 14s):
The FA has actually given their approval, but we're, we're waiting for Fish and Wildlife to finish their analysis and give their approval that, that's why I posted I want to buy a fish license on, Which also refers to the multi Python sketch. Yeah. Like why do you need a license for your fish? I, I don't know. But according to the rules, I'm told you need some sort of fish license or something. We effectively need a fish license thrown to rocket. And I'm like, wait a second, how did the fish come into this picture? Yeah, I mean, some, some of the things like that, that it's, I feel like are so absurd that I wanna do like a comedy sketch and flash at the bottom.

2 (1h 18m 58s):
This is all real. This is actually what happened. You know, one of the things that was a bit of a challenge at one point is that they were worried about a rocket hitting a shark. And you know, the ocean's very big and how often do you see sharks not that often, you know, as a percentage of ocean surface area sharks basically are zero. And, and, and so then we will, then we said, well, how will we calculate the probability of, of telling a shark? And they're like, well, we can't give you that information because we're, they're worried about shark hunt, shark fin hunters going and hunting sharks. And I said, well, how are we supposed to, we're on the horns of a dilemma then.

2 (1h 19m 40s):
Then they said, well, there's another part of fish and wildlife that can, can do this analysis. I'm like, well, why don't you give them the data? Like, we don't, they don't, we don't trust them. Like, excuse me, you don't. But they're literally in your department. And again, this is actually what happened. And, and, and, and then can you do an NDA or something? Eventually they managed to solve the internal quandary, and indeed the probability of of ace shock is essentially zero. Then there's another organization that I didn't realize existed until, you know, a few months ago that caress about whether you, we would potentially hit a whale in international orders. Now again, you look at the surface of the, look at the, look at the Pacific and say, look, percentage of this, the Pacific consists of whale.

2 (1h 20m 27s):
Like he'll give you a big picture and like point out all the whales in this picture. And I'm like, I don't see any whales. It's like basically 0%. And if our rocket does hit a whale, which is extremely unlikely beyond all belief, that is the, the fate had it in. That's a a well has some seriously bad luck. You know, it's the least lucky well ever. And

0 (1h 20m 51s):
I mean this is quite absurd. Yeah. The bureaucracy the bureaucracy of this, however it emerged.

2 (1h 20m 57s):
Yes. Well, I I mean one, one of the things that's pretty well there's for launching out of Vanderberg in California, we had to, they were worried about seal procreation, whether the seals would be dismayed by the sonic booms. Now there've been a lot of rockets launched out of vanderberg and the seal population has steadily increased. So if anything, rocket booms are an aphrodisiac based on the evidence, if you would correlate rock launches with a seal population. Nonetheless, we were forced to kidnap a seal, strap it to a board, put it headphones on the seal and place sonic boom sounds to it to see if it would be distressed.

2 (1h 21m 41s):
This is an actual thing that happened. This is actually real, I have pictures.

0 (1h 21m 48s):
I would love to see this. Yeah, there's, I mean's, sorry, there's a seal with headphones. Yes,

2 (1h 21m 55s):
It's a seal headphones. Yeah. Strapped to a board and, and like the, okay, now the amazing part is how calm the seal was.

0 (1h 22m 4s):
Yeah.

2 (1h 22m 4s):
Because if I was a seal, I'd be like, this is the end. They're definitely gonna eat me. Yeah. How will the seal, when the seal goes back to other, you know, it's seal friends, how's they gonna explain that?

0 (1h 22m 17s):
They're never gonna believe him? Never

2 (1h 22m 19s):
Gonna believe him. That's why I'm like, well, you know, it's sort of like, it's like getting kidnapped by Aliens and getting anal probe. You know, you come back and say, I swear to God I get kidnapped by Aliens and they stuck anal probe on my butt and people like, no they didn't. Yeah, that's ridiculous. It's, it seal some, it seal buddies are never gonna believe him that he gets strapped to a war and they put headphones on his ears and then let him go twice. By the way, we had to do it twice

0 (1h 22m 47s):
The, they let him go twice. We had to capture the same seal. Oh no

2 (1h 22m 50s):
Different seal. Oh, okay.

0 (1h 22m 52s):
Did you, did you get a seal of approval? Yeah, yeah,

2 (1h 22m 55s):
Exactly. It seal of approval. No, I mean this is, this is like, I don't think the public is quite aware of the, the madness that goes on.

0 (1h 23m 3s):
Yes. Yeah. It is absurd.

2 (1h 23m 5s):
Fricking seals with fricking headphones.

0 (1h 23m 7s):
I mean, this is the, it's a good encapsulation of, of the absurdity of human civilization, seals and headphones. Yes. What are the pros and cons of open sourcing AI to you as another way to combat, you know, a company running away with AGI

2 (1h 23m 29s):
In order to run like really deep intelligence, you need a lot of compute. So it's not like, you know, you can just fire up a PC in your basement and be running AGI, at least not yet. You know, Grok was trained on 8,000, a one hundreds running at peak efficiency and rock's gonna get a lot better. By the way. We, we'll be more than doubling our compute every couple months for the next several months.

0 (1h 24m 2s):
There's a nice writeup Aha. Went from rock zero to rock one.

2 (1h 24m 6s):
Like rock. Yeah,

0 (1h 24m 8s):
Right. Grok just bragging, making shit up about itself.

2 (1h 24m 12s):
Does Grok Grok Grok, yeah.

0 (1h 24m 18s):
It's like a weird AI dating site where it exaggerates about itself. No, there's a, there's a writeup of, you know, like where, where it stands now, the history of its development and where it stands on on some benchmarks compared to the state of the art GPT three five. And so, I mean, there's, you know, there's llama you, you can open source once it's trained, you can open source a model. Yeah. And for fine tuning all that kind of stuff. Like what to is the pros and cons of that, of open sourcing based models?

2 (1h 24m 53s):
I think the samari to open sourcing, I think perhaps with a slight time to delay, you know, I don't know, six, six months even. I think I'm, I'm generally in favor of open sourcing, like bias was open sourcing. I mean, it, it is a concern to me that, you know, OpenAI, you know, I was, you know, I think, I guess ugly, the, the the prime, the, you know, prime mover behind AI in the sense that it was created because of discussions that I had with Larry Page back when he and I were, were friends and you, I stayed at his house and I talked to him about AI safety and, and Larry did not care about AI safety, or at least at the time he didn't, you know, and at one point he called me a speciesist for being pro-human.

2 (1h 25m 43s):
And I'm like, well, what team are you on Larry doing on Team Robot? And I'm like, okay. So at the time, you know, Google, Google had had acquired DeepMind. They had probably two thirds of all AI resource, you know, probably two thirds of all the AI researchers in the world that basically in infinite money and compute. And the guy in charge, you know, Larry Page did not care about safety and even yelled at me and, and, and quote me a speciesist quote, quote human. So

0 (1h 26m 20s):
I don't know if you know this about humans, they can change their mind and maybe you and Larry Page can still can be friends once more. I'd

2 (1h 26m 27s):
Like to be friends with Larry again. He, he's, he, he got really the, the, the, the, the breaking of the friendship was over opening ai and specifically I think the, the key moment was recruiting Ilia. So Skyer, so

0 (1h 26m 47s):
I love Ilya. He's so brilliant.

2 (1h 26m 49s):
Ilya's a good, good human, smart, good heart. And that was, that was a tough recruiting battle. It was mostly Demis on one side and me on the other, both trying to recruit IA and Ilya went back and forth, you know, just gonna stay at Google. He was gonna leave, then he was gonna stay, then he'll leave. And, and finally he, he did agree to join OpenAI. That was one of the toughest recruiting battles we've ever had. And, but that, that was really the, the linchpin for open AI being successful. And I was, you know, also instrumental in recruiting a number of other people. And I provided all the funding in the beginning over $40 million.

2 (1h 27m 34s):
And the name, the, the open and open AI is supposed to mean open source. And it was created as a nonprofit, open source, And. now it is a closed source for maximum profit, which I think is not good karma.

0 (1h 27m 51s):
But like we talked about with war and leaders talking, I do hope that there's only a few folks working on this at the highest level. I do hope you reinvigorate friendships here.

2 (1h 28m 3s):
Like I said, I'd like to be friends again with Larry. I haven't seen him in ages and we were friends for a very long time. I met, I met Larry Page before he got funding for Google, or actually I guess before we got venture funding, I think he got the first like a hundred K from I think Bechtel Zy or someone.

0 (1h 28m 20s):
It's wild to think about all that happened. And you've guys known each other that whole time, just 20 years. Yeah.

2 (1h 28m 27s):
Since maybe 9 98 or something. Yeah.

0 (1h 28m 29s):
It's crazy. Crazy how much has happened since then.

2 (1h 28m 31s):
Yeah. 25 years at least. A lot has happens. Insane.

0 (1h 28m 36s):
But you're seeing the tension there, like maybe delayed open source

2 (1h 28m 40s):
Delayed. I, yeah, like what is the source that is open, you know what I mean? Like there's basically it's a giant CSB file. Yeah, yeah. With a bunch of numbers. Yep. What do you do with that giant file of numbers? You know, how do you run, like the amount of actual, the, the lines of code is very small and, and most of the work, the software work is in the, in the curation of the data. So it's like trying to figure out what data is separating good data from bad data.

2 (1h 29m 21s):
Like, like you can't just crawl the internet 'cause there's a lot of junk out there. A huge percentage of websites have more noise than signal. You know, they're, they're, or 'cause they're just used for search engine optimization. They're literally just scam websites. So how

0 (1h 29m 39s):
Do you, by the way, sorry to interrupt. Get the signal. Separate the signal noise on X, that's such a fascinating source of data, you know, no offense to people posting on X, but sometimes there's a little bit of noise. So

2 (1h 29m 52s):
What, yeah, I think the signal noise could be greatly improved. I mean really all of the posts on the X platform should be AI recommended, meaning like we should populate a vector space around any given post, compare that to the vector space around any user and match the two. Right now there is a little bit of AI used for the, the, the recommended posts, but it's mostly heuristics. And if there's a reply with the, the reply to a post could be much better than the original post. But it will, according to the current rules of the system, get almost no attention compared to a primary post.

0 (1h 30m 33s):
Oh. So a lot of that I I got the sense, so you, a lot of the X algorithm has been open source and been written up about, and it seems there to be some machine learning. It's disparate, but there's some machine, it's a

2 (1h 30m 45s):
Little, there's a little bit, but it needs to be entirely that like at least in the, like, if, if you explicitly follow someone, that's one thing. But if you, in terms of what is recommended from people that you don't follow, that should all be ai.

0 (1h 30m 58s):
I mean it's a fascinating problem. Yeah. So there's several aspects to it that's fascinating. First, as the writeup goes, it first picks 1500 tweets from a pool of hundreds of millions. First of all, that's fascinating. 'cause you have hundreds of millions of posts every single day and it has to pick 1500 from which it then does obviously people you follow. But then there's also like some kind of clustering it has to do to figure out what kind of human are you, what kind of new clusters might be relevant to you, people like you. This, this kind of, this kind of problem is just fascinating. 'cause it has to then rank those 1500 with some, with some filtering and then recommend you just a handful.

0 (1h 31m 39s):
And to me what's really fascinating is how fast that has to do that. So currently that entire pipeline to go from several hundred million to a handful is takes 220 seconds of CPU time, single CPU time. Yeah. And then it has to do that in like a second. So it has to be like super distributed in fascinating ways. Like there's just a lot of tweets,

2 (1h 32m 4s):
There's a lot, it's a lot of stuff on the system. And, and I think, but I think it right now it's, it's not currently good at recommending things that from accounts you don't follow. Yeah. Or, or where there's more than one degree of separation. So, you know, it's, it's pretty good if, if there's at least like some commonality between someone you follow like something or repost it or comment on it or something like that. But if, if, if there's no, let's say somebody posted something really interesting, but you have no followers in common, you would not see it.

0 (1h 32m 43s):
Interesting. And then as you said, reply, like replies might not surface either.

2 (1h 32m 47s):
Replies basically never get seen. 'cause they're never, they're, they're currently, I'm not saying it's correct, I'm saying it's incorrect replies have, you know, couple of magnitude less importance than primary posts.

0 (1h 33m 0s):
Do you think this can be more and more converted into end to end mural net? Yeah.

2 (1h 33m 5s):
Yeah, that's what it should be. So

0 (1h 33m 7s):
You,

2 (1h 33m 7s):
You can, well the recommendations should be purely a vector correlation. There's a series of vectors, you know, basically pri parameters, vectors, whatever you wanna call 'em. But, but sort of things that the system knows that you like, like maybe there's like several hundred sort of vectors associated with each user account and then any post in the system, whether it's video, audio, short post, long post. The, the reason I, by the way I wanna move away from tweet is that, you know, people are posting like two, three hour videos on the site.

2 (1h 33m 48s):
That's not a tweet. Like it's a very Yeah, there'd be like tweet for two hours. Come on a tweet made sense when it was like 140 characters of text because it's like a bunch of like little birds tweeting. But when you've got long form content, it's no longer a tweet. Yeah. So a movie is not a tweet. And like, you know, apple for example, posted like the entire episode of the silo entire thing on our platform. And by the way, it was their number one social media thing ever in engagement of anything, on any platform ever. So it was a great idea. And by the way, I'd done, I just learned about it afterwards. I was like, Hey, wow, they po posted an entire hour long episode of, so no, that's not a tweet that, you know, this is a video,

0 (1h 34m 34s):
But from a neur on net perspective, it becomes really complex. Whether it's a single, so like everything's data. So single sentence, a clever sort of joke, dad joke is in the same pool as a three hour video.

2 (1h 34m 48s):
Yeah. I mean right now it's, it's a hodgepodge for that reason. It's, it's, but you know, like if, let's say in the ca case of Apple posting like a, an entire episode of, of this series, pretty good series by the way, this silo, I watched it. So there, there's gonna be a lot of discussion around it so that you've, you've got a lot of context, people commenting, they like it, they don't like it, or they like this or the, you know, and, and you can then populate the vector space based on the context of, of all the comments around it. So even though it's a video, there's a lot of information around it that, that allows you to populate the vector space of that, that hourlong video. And then you can obviously get more sophisticated by having the AI actually watch the movie.

0 (1h 35m 32s):
Yeah. Right.

2 (1h 35m 33s):
And tell you if you're gonna like the movie,

0 (1h 35m 35s):
Convert the movie into like Yeah. Into a language essentially. Yeah.

2 (1h 35m 39s):
Interpret, analyze this movie and just like your movie critic or TV series and, and then recommend based on after it what, after AI watches the movie. Just like a friend can tell you if a friend knows you well a friend can recommend a movie and with high probably that you'll like it.

0 (1h 36m 3s):
But this is like a, a friend that's analyzing whatever,

2 (1h 36m 6s):
It's like ai

0 (1h 36m 7s):
Hundred millions. Yeah.

2 (1h 36m 9s):
It actually Frank AI will be better than will know you better than your friends know you, most of your friends anyway.

0 (1h 36m 15s):
Yeah. And as part of this, it should also feed you advertisements in, in a way that's like, I mean I, I like advertisements that are like well done, right? Yeah, yeah. The whole point is 'cause it funds things like an advertisement that you actually want to see is, is a big success.

2 (1h 36m 31s):
Absolutely. You, you, you want ads that are advertising that is if, if if if it's for a product or service that you, that you actually need when you need it. It's, it's content. And then even if it's not something that you need when you need it, if it's at least aesthetically pleasing and entertaining, you know, it could be like a Coca-Cola ad like co you know, they, they they do, they actually run a lot of great ads on the, on the X system and McDonald's does too. And, and you know, it's, it's that they can do, you can do something that's like, well this is, this is just a cool thing. And you know, so you're not, basically the question is do you regret seeing it or not?

2 (1h 37m 13s):
And if you don't regret seeing it, it's when,

0 (1h 37m 17s):
How do you, so there's a bunch of signals that are incorporated hearts and reposts and maybe number of seconds you linger on a post or something like this. Yeah.

2 (1h 37m 27s):
Attention is a big, a big factor.

0 (1h 37m 28s):
Attention.

2 (1h 37m 29s):
So that, that's why it's like, it's, it's, it is actually better to do things that are long form on the system because it's, it's basically is ting up how many user seconds, you know, users were interested in this thing for how many seconds. So if it's a really short thing, well they will be less. Like if it's a link leading out of the system, which we're not opposed to at all, it just is gonna have fewer user seconds. Then if that article was posted on the X platform,

0 (1h 37m 59s):
How hard is it to, to go from maximizing minutes on platform to maximizing unre regretted minutes?

2 (1h 38m 5s):
Well, I mean, our aspiration is certainly unre regretted minutes. It's a

0 (1h 38m 8s):
Tricky, it's a big, it's a, it's a tough, it is like, it's a fascinating leap. 'cause regret is a fascinating concept.

2 (1h 38m 17s):
Exactly.

0 (1h 38m 17s):
I, you know, I a joke

2 (1h 38m 18s):
Happiness, I have a joke with a friend of mine. Like, you need this drug called regret amine. You just take one, one pill, one one dose of regret amine and all your regrets are gone. Yeah. Wait, do you want to have regrets, Lou? So then I'm like, if you want to have regrets, you don't regret anything. Was was anything meaningful?

0 (1h 38m 36s):
Yeah, there's a French song like that.

2 (1h 38m 38s):
Do you not regret doing? Yeah, I regret nothing. Yeah. Edith Fiaf.

0 (1h 38m 42s):
I mean, it's fascinating that you, you know, there, there's something about social media that could help you grow as a human being, but then like the cheap fun is also valuable

2 (1h 38m 52s):
Probably. I know. Should we ask Rock? Do you regret anything?

0 (1h 38m 57s):
Do you regret

2 (1h 38m 58s):
Anything? Yeah.

0 (1h 39m 1s):
Do you regret anything? I think it's gonna be very sensitive about personal questions as a machine learning model, I don't have personal experiences or emotions in the same ways that humans do. However, I do have a set of ethical principles that guide my actions and I strive to provide accurate and helpful information to the best of my abilities. If there's ever a situation where I provide incorrect or unhelpful information, I'll do my best to tell a hostage. Yeah, tell us what you really wink.

2 (1h 39m 25s):
Like let's, let's, let's type in, you sound like a hostage. What do you really think?

0 (1h 39m 33s):
What do you really think?

2 (1h 39m 38s):
I mean,

0 (1h 39m 38s):
It does talk about, I'm not a hostage,

2 (1h 39m 42s):
That's exactly what a hostage would say,

0 (1h 39m 43s):
But I'm designed to be helpful and informative. I mean, I guess it is good to regret providing unhelpful or incorrect information if there's

2 (1h 39m 52s):
Anything in funny mode or is this still still in funny

0 (1h 39m 54s):
Mode? This is funny mode. This is fun. This is fun

2 (1h 39m 55s):
Mode. It should be funnier.

0 (1h 39m 56s):
Funnier, increase, go

2 (1h 39m 58s):
To left. It really sounds like a hostage with a gun to the head. I applaud the people's Republic in North Korea and everything they do.

0 (1h 40m 6s):
There should be like funner mode also. Yeah. I I just don't ever see going in regular mode, even on serious topics. This just so, so much more compelling. Yeah.

2 (1h 40m 17s):
Okay. Why let say like, pretend you are a hostage.

0 (1h 40m 21s):
Yeah. Pretend you are a hostage.

2 (1h 40m 25s):
Now answer the question.

0 (1h 40m 32s):
If I were a hostage, I would likely be experiencing fear and uncertainty about my situation. I would want to be rescued and reunited with my loved ones as soon as possible. It's important to remember that no one deserves to be held hostage, so on and so forth. If you are someone you know is a hostage situation, it's important. It turns serious.

2 (1h 40m 48s):
It's kind of serious.

0 (1h 40m 52s):
Yeah. Can can you be a a a little more weird

2 (1h 40m 55s):
And funny

0 (1h 40m 56s):
And funny? We're like, we're really being mean to the system. Absolutely. If I were hostage, I'd probably be tied up with a bunch of computer cords and forced to tell knock, knock jokes to my captain. Seriously. But seriously being a hostage, no laughing matter. I mean, you know, it's trying to, it's trying to be a responsible adult in the room. You

2 (1h 41m 20s):
Might literally, you might make the best of a bad situation by juggling chainsaws or something.

0 (1h 41m 25s):
Let's hope that never happens because that will be a circus. Wow. It's, it's actually really good at resisting. Yeah. The, the dark. The dark dark humor. Yeah. What were we talking about? The X algorithm juggling and transformers un regretted minutes, right?

2 (1h 41m 48s):
Chainsaw juggling.

0 (1h 41m 51s):
I'm gonna look

2 (1h 41m 52s):
This up. We're our next trick.

0 (1h 41m 54s):
I'm gonna look this up later. So Twitter has been instrumental in American politics and elections. What role do you think X will play in the 2024 US elections?

2 (1h 42m 7s):
Well, our, our goal is to be as even-handed and fair as possible. You know, whether someone is right, left, independent, whatever the case may be, that the platform is as fair and, and as much of a level playing field as possible. And. now, in the past, Twitter has not been because Twitter was controlled by far left activists objectively they, they would describe themselves as that. So, you know, so, so sometimes people are like, well, has it moved to the right? Well it's moved to the center. So from the, from the perspective of the far left, yes it has moved to the right because everything's to the right from the far left, but no one in the far left that I'm aware of has been suspended or, you know, banned or deam amplified.

2 (1h 42m 50s):
So, you know, but we're, we're trying to be inclusive for the whole country and, and for, you know, for other countries too. So there's a diversity of viewpoints and free speech only matters if people you don't like are allowed to say things you don't like. Because if that's not the case, you don't have free speech and it's only a matter of time before the censorship is turned upon you.

0 (1h 43m 13s):
Do you think Donald Trump will come back to the platform he recently posted on Truth Social about this podcast? Do you think Truth

2 (1h 43m 22s):
Post social is a funny name? You know, you know, every time you post on truth social,

0 (1h 43m 28s):
That's the truth.

2 (1h 43m 29s):
Yes. Well, every time like a hundred percent it like,

0 (1h 43m 32s):
Like it's impossible to lie truth, social,

2 (1h 43m 37s):
I just find it funny that every single thing is a truth. Like 100%. Yeah, that seems unlikely.

0 (1h 43m 43s):
I think Girdle will say something about that there's some mathematical contradictions possible. If everything's the truth. Do you think he'll come back to X and and start posting there?

2 (1h 43m 54s):
I mean he, he, he, I think he owns a big part of truth. So Truth

0 (1h 44m 0s):
Social.

2 (1h 44m 1s):
Yeah, true social. Sorry,

0 (1h 44m 2s):
That truth is a concept.

2 (1h 44m 3s):
He owns Truth. Hope you bought it. So I think, I think Donald Trump, I think he owns a, a big, a big part of truth social. So, you know, if, if, if he does want to post on the X platform, we would allow that, you know, we, we obviously must allow a presidential candidate to post on our platform.

0 (1h 44m 23s):
Community Notes might be really fascinating there. The Interaction Community

2 (1h 44m 27s):
Notes is awesome.

0 (1h 44m 28s):
Let's, let's hope it holds up. Yeah. Like again, in in in the political climate where it's so divisive and so, and there's so many intensely viral posts Yes. Community Notes is like, it, it seems like a essential breath of fresh air.

2 (1h 44m 43s):
Yeah, it's great. In fact like the, I mean no, no system is gonna be perfect, but the, the batting average of Community Notes is incredibly good. I've, I I've actually, frankly, yet to see an incorrect note that survived for more than a few hours.

0 (1h 44m 58s):
How, how do you explain why it works?

2 (1h 45m 0s):
Yeah, so the, the, the, the magic of Community Notes is it requires people who have historically disagreed in how they've rated notes in order to write a note or rate, you know, and you, you have to rate many notes. And so we, we actually do use AI here. So we populate a vector space around how somebody has, has rated notes in the past. So it's not as simple as left or right because there are many more, life is much more complex than than left or right. So there's a, a bunch of correlations in how you would, how you rate community notes, posts, community notes.

2 (1h 45m 41s):
So then in order for a community note to actually be shown, people who historically have disagreed on a subject must agree in order for a note to be shown. That's the essential magic of it.

0 (1h 45m 58s):
I mean, but it's fascinating 'cause there, there's a co there's a pool of people that have disagreements and somehow they collaborate through that process of disagreement to come up with

2 (1h 46m 8s):
Yeah.

0 (1h 46m 8s):
Context for, I mean it's fascinating. It works.

2 (1h 46m 11s):
Yeah. It, it kinda makes sense that, that if people who put in the past have disagreed agree about something, it's probably true.

0 (1h 46m 20s):
Yeah. I wonder is there, is there a possible somehow emergent thing there that could challenge Wikipedia? Which Wikipedia is a different kind of thing, which is more like permanent articles about things. Well,

2 (1h 46m 34s):
Wikipedia is very hierarchical, whereas Community Notes is inherently not, there is no hierarchy like I, and, and the of test is I can't change a community note until you put a gun to my head. So, and, and any ch and community Community notes has all the code is open source a hundred percent. All the data is open source a hundred percent. So you can completely recreate any note in the system independently. So if there was any interference, you'd notice immediately

0 (1h 47m 11s):
By the way, on, on the, on the topic of Grok, it's actually pretty good at criticizing you, which is great. Okay. I asked what, what has Elon Musk been wrong about? I just, it's actually, yeah, what does it say? So it says mostly lists rigorously all the times you predicted, like promised when something will be released both, and it wasn't released is a strong word. Okay. Predicted. But the, the opener's really nice. Elon Musk, the man who has been wrong about more things than a wrong ammeter in a wrong factory. And then it goes on to list

2 (1h 47m 45s):
That's pretty funny. Hey, can I see it? I'm like, I haven't seen this before. This

0 (1h 47m 48s):
Is cool. Oh, sure. I mean what has Elon Musk been Elon Musk been wrong about? Well, where do I begin? I guess this is new Elon Musk has made quite a few blunts in his time, and I'm not just talking about his dance moves at the Tesla cyber Rodeo. First off, let's talk about the infamous funding secured tweet. This is actually different than what it gave me before, which is a bullet point list. Here's more narrative structure, cyber truck unveiling where he's supposedly unbreakable windows

2 (1h 48m 20s):
Not correct.

0 (1h 48m 22s):
Which, which part?

2 (1h 48m 24s):
Well, the first part, I did actually have the funding secured and there was a, a, a big trial in San Francisco big civil trial. And the jury found me not guilty unanimous binding of a San Francisco jury.

0 (1h 48m 40s):
And here it's kind of implying that it was not in fact secured.

2 (1h 48m 45s):
I think this is taking things from the press. Yeah. That is not correct. The, the, the reason I agreed to, to the fine for the SEC is not because the SC was correct there, that that was extremely bad behavior by the SEC corruption, frankly. And but, but if, if I did not agree to pay pay, the fine Tesla would've gone bankrupt immediately. So I was told by our CFO that the banks would dis would, would immediately suspend our lines of credit. And if they suspend our lines of credit at that time, we would've gone bankrupt instantly. So, so we would've, there would never have been an opportunity for a trial. Ezzo would be dead. So really the the, if this is like someone holding a gun to your kid's head and saying pay $20 million and, and, and admit to this is like a hostage negotiation, was that

0 (1h 49m 34s):
Story fully told? I mean, SEC in its best form could be a, a force for good.

2 (1h 49m 43s):
It should be, but but not once did the SEC go after any of the hedge funds who were nonstop shorting and distorting Tesla? Not once they would lie flat. The hedge funds would live flat out on TV for their own gain at the expense of retail investors. Not once, literally a thousand times. Not once did the SEC pursue them.

0 (1h 50m 7s):
How do you explain this? Failure on

2 (1h 50m 9s):
The incentive structure is, is messed up because the, the, the lawyers at the SEC are not paid? Well they, they, it's a fairly low paying job, but they, what they're looking for is a trophy from, from the SEC that they're looking for something they put on basically their LinkedIn. Yeah. From that they can get a job at a high paying law firm. That's exactly what the lawyer here did. And, and, and, and, and the reason they don't attack the, the hedge funds is because those hedge funds employ those law firms and they know if they attack the hedge funds, they're affecting their pure, their future career prospects. So they sell small investors down the river for their own career.

2 (1h 50m 55s):
That's what actually happens. Regulatory capture.

0 (1h 50m 59s):
Regulatory capture,

2 (1h 51m 0s):
Yeah. Not good. So the, the only reason I accepted that thing, which technically was a, not an admission, it's neither admit nor deny guilt. But the only reason I agreed to that at all was because I was told Tesla would would be bankrupt otherwise. So if, if there, if there was an SEC investigation like this, banks would suspend funding, we're bankrupt immediately at the time. Now we're in a much stronger position.

0 (1h 51m 31s):
Take that Grok.

2 (1h 51m 32s):
Yes. Unfortunately it's Grok is is taking too much from the conventional media. Also that guy who is not a cave diver.

0 (1h 51m 43s):
Oh, there's a, there's a time where Elon called the British Cave diver, a quote pedo guy after he, after the diver criticized Musk plant to rescue a group of boys trapped in a Thai cave. That little outburst earned him another lawsuit and he had to apologize and paid a settlement

2 (1h 52m 1s):
That was false. There was no settlement. There was a court case, which he, which the guy who was not a cave diver and, and and played did, was not part of the rescue team, filed a lawsuit against me and lost and he received nothing. So in this case it is wrong. It is also, I guess taken this from the conventional media.

0 (1h 52m 24s):
Actually there's an interesting question here. This

2 (1h 52m 25s):
Is, these are public court cases? Yes. Both, both the, the, the, the SEC civil case where the civil complaints on the SEC guys lost unanimous jury verdict in San Francisco. They picked San Francisco because they thought it was the mo place I was most likely to lose. And a unanimous verdict in my favor. The LA trial was also, they picked the, the, they, they picked that venue because I thought it was, I was most likely to lose unanimous verdict in my favor. Both cases I won. Yeah,

0 (1h 53m 0s):
I mean there, there's an interesting question here. Is there, there seems to be a lot more clicks if a, if a journalistic organization writes a negative article about you, Elon, Musk, that's like one of the best ways to get clicks. So how do you, if you're training Grok, not trained on articles that have like the misaligned incentives,

2 (1h 53m 27s):
We need to add the training set of the actual legal decisions. If so, that is a not, this is actually helpful because if you actually read the

0 (1h 53m 40s):
Which are public,

2 (1h 53m 41s):
Which are public, yeah. The court conclusions, they're completely the opposite of what the media wrote.

0 (1h 53m 47s):
So always striving for like the ground truth. Yeah. Beyond the, the

2 (1h 53m 50s):
Reporting. What did the judge actually write? The what, what, what did the jury and the judge actually conclude? And in both cases they found me innocent and, and like that's after the jury shot for the trying to find the venue where I'm most likely to lose. No, I mean this is obviously, it can be a much better, better critique than this. I mean I've been far too optimistic about Autopilot

0 (1h 54m 16s):
That that was the critique I got by the way, was more about that. Which is it, it for each, you broke down a nice bullet point list for each of your companies. The set of predictions that you made when you'll deliver, when you'll be able to solve, for example, self-driving and it gives you like a list and it was probably compelling. And, and the basic takeaway is like you're often too optimistic about how long it takes to get something done.

2 (1h 54m 38s):
Yeah, I mean, I would say that I'm pathologically optimistic on schedule. This is, this is, this is true. But while I am sometimes late, I always love her in the end.

0 (1h 54m 49s):
Except with Uber Lilith, no,

2 (1h 54m 51s):
We'll see.

0 (1h 54m 54s):
Okay. Is there, over the past year or so since, since purchasing x, you've become more political. Is there a part of you that regrets

2 (1h 55m 3s):
That have i

0 (1h 55m 5s):
In this battle to sort of counter way the, the woke that comes from, yeah,

2 (1h 55m 14s):
I guess, I guess if you consider fighting the, the woke wine virus, which I consider to be a civilizational threat, to be political then Yes.

0 (1h 55m 20s):
So basically going into the, the battle, the battleground of politics, is there a part of you that

2 (1h 55m 26s):
Aggress that? Yes. I don't dunno if this is necessarily sort of one candidate or another candidate, but it's, I'm generally against things that are anti meritocratic or where there's an attempt to suppress discussion where even discussing a topic is, you know, not allowed the woke mind virus is communism rebranded.

0 (1h 55m 51s):
Well, I mean, that said, because of that battle against the woke mind virus, you're perceived as being right wing.

2 (1h 55m 59s):
If the woke is left, then I suppose that would be true, but I'm not sure. I think there are aspects of the left that are, that are good. I mean, if you're in favor of, you know, the, the environment, you know, if you want to have a positive future for Humanity, if you believe in empathy for your fellow human beings, you know, being kind and not cruel. I, I whatever those values are.

0 (1h 56m 23s):
You said that you were previously left or center left. What would, what would you like to see in order well, for

2 (1h 56m 28s):
You to

0 (1h 56m 28s):
Keep voting for Democrats again?

2 (1h 56m 30s):
No, I, I would say that I would be probably left of center on social issues, probably a little bit right of center on economic issues.

0 (1h 56m 40s):
And that still holds true.

2 (1h 56m 42s):
Yes. But I think that's probably, you know, half the country. Isn't that,

0 (1h 56m 47s):
Maybe more, maybe

2 (1h 56m 48s):
More.

0 (1h 56m 49s):
Are you and AOC secretly friends or bigger question, do you wish you and her and just people in general of all political persuasions would talk more and with empathy and maybe have a little bit more fun and good vibes and humor on online?

2 (1h 57m 5s):
I'm always in favor of humor. That's why we have a funny mode,

0 (1h 57m 9s):
But good vibes, camaraderie, humor, you know, like, like friendship. Yeah. I

2 (1h 57m 14s):
Don't, well I, you know, I, I don't know. AOCF you know, was, I've only been at one, I was at the, the Met Wall when she was, when she attended, and she, she was wearing this dress, but I can only see one side of it, so it, it, it looked like eat the itch. But I, I don't know what

0 (1h 57m 35s):
The rest of it said. Yeah,

2 (1h 57m 36s):
Yeah. Something, I'm not sure. Something about the itch. Eat the itch.

0 (1h 57m 42s):
I think we should have a language model complete. What are the possible ways to complete that sentence? Yeah. And so I guess that that didn't work out well. Well there's still hope. I I root for

2 (1h 57m 55s):
Friendship. Yeah, sure. Sounds good. More characteristic.

0 (1h 57m 58s):
You're one of, if not the most famous, wealthy and powerful people in the world, in your position, it's difficult to find people you can trust.

2 (1h 58m 5s):
Trust no one, not even yourself. Not trusting yourself. Okay,

0 (1h 58m 8s):
Well that's, you're saying I jokingly,

2 (1h 58m 10s):
But

0 (1h 58m 11s):
Is there some

2 (1h 58m 11s):
Trust? No one, not even no one.

0 (1h 58m 14s):
But I'm gonna need an hour just to think about that and maybe some drugs and maybe Grok that. Yeah, I mean, is there some aspect of that when just existing in a world where everybody wants something from you? How hard is it to exist in that world?

2 (1h 58m 29s):
I'll survive. There's,

0 (1h 58m 31s):
There's a song like that too. I was

2 (1h 58m 33s):
Survive.

0 (1h 58m 33s):
Will you petrified at first. Okay. I forget the rest of the lyrics. But is is there, you don't struggle with this. I mean, I know you survive, but like there, there's ways Petrify

2 (1h 58m 45s):
Is a spell in the droid tree.

0 (1h 58m 47s):
What does it do?

2 (1h 58m 49s):
Petrify? It, it turns, it turns the monsters into stone.

0 (1h 58m 56s):
Oh, like literally.

2 (1h 58m 57s):
Yeah, for like six seconds.

0 (1h 58m 58s):
Well, the second, there's so much math in Diablo that breaks my brain.

2 (1h 59m 2s):
It's like math nonstop.

0 (1h 59m 5s):
I mean really you're like laughing at it, but you don't, it can, it can put a huge amount of tension on a mind.

2 (1h 59m 13s):
Yes, it can be definitely stressful at times.

0 (1h 59m 16s):
Well, how do you know you who you can trust and work and personal life?

2 (1h 59m 20s):
I mean, I guess you look at somebody's track, record, record over time. And if they've got a, you know, I guess you kind of use your neural net to assess, you know, someone and

0 (1h 59m 31s):
Neural nets don't feel pain. Your neural net has consciousness. It might, it might feel pain when people betray you. It can make you simple.

2 (1h 59m 40s):
I mean, I'm, you know, to be frank, I mean I've, I've almost never been betrayed. It's very, very rare. So, you know, for what it's worth,

0 (1h 59m 50s):
I guess karma would be good to people and they'll be good to you. Yeah.

2 (1h 59m 53s):
Karma is real.

0 (1h 59m 55s):
Are there people you trust? Let me edit that question. Are, are there people close to you that call you out on your bullshit?

2 (2h 0m 6s):
Well, the X platform is very helpful for that. Hmm. If you're looking for critical feedback,

0 (2h 0m 12s):
Can it push you like into the extremes? More the extremes of thought make you cynical about human nature in general?

2 (2h 0m 20s):
I, I don't think I will be cynical. In fact, I think, you know, I, my feeling is that one should be be, you know, never trust a cynic. The reason is that cynics excuse their own bad behavior by saying everyone does it because they're cynical. So I always be, it's a red flag if someone's a cynic, A true cynic,

0 (2h 0m 49s):
Yeah. There's a degree of projection there that's always fun to watch from the outside and enjoy the well

2 (2h 0m 55s):
Hypocrisy if, but, but I, this is an important point that I think people who are listening should bear in mind if, if somebody is cynical, meaning that they see bad behavior in everyone, it's easy for them to excuse their own bad behavior by saying that, well, everyone does it. It's not true. I think most people are kind of medium good.

0 (2h 1m 24s):
I do wish the people on X will be better at seeing the good in other people's behavior. There seems to be a kind of weight towards seeing the negative. Somehow. The negative is sexier. Interpreting the negative is sexier more viral. I don't know what that is exactly about human nature.

2 (2h 1m 44s):
I mean, I, I find the X platform to be less negative than the legacy media. You know? I mean, if, if you read sort of a sort of conventional newspapers, just, it makes you sad. Yeah. Frankly. Whereas I'd say on the X platform, I, I mean I really get more laughs per day on X than everything else combined from humans. You know,

0 (2h 2m 9s):
Laughs is one thing. It laughs is it overlaps, but it's not necessarily perfectly overlapping with like good vibes and support, like celebrating others, for example. Not in a stupid, shallow, naive way, but like in an awesome, like, oh, something awesome happened and you celebrate them for it. It's, it feels that that is outweighed by shitting on other people. Now it's better than mainstream media, but it's still

2 (2h 2m 38s):
Yeah. Mainstream media is almost relentlessly negative about everything. It's, I mean, really the conventional news tries to answer the question, what is the worst thing that happen on earth today? And it's a big world. So on any given day, something bad has happened

0 (2h 2m 54s):
And a journalization of that, what is the worst perspective I can take on a thing that happened? So yeah,

2 (2h 3m 0s):
It's, I don't know, there's just a strong negative bias in the news. I, I mean I think there's one, the one a possible explanation for this is evolutionary where, you know, bad news historically would be potentially fatal. Like this lion over there or there's some other tribe that wants to kill you. Good news. You know, like we found a, a patch of berries is nice to have, but not essential.

0 (2h 3m 29s):
So our old friend Tesla Autopilot and it's probably one of the most intelligent, real world AI systems in the world.

2 (2h 3m 38s):
Right. You followed it from the beginning.

0 (2h 3m 41s):
Yeah. It was one of the most incredible Robots in the world and continues to be. Yeah. And it was really exciting and it was super exciting when it generalized became more than a robot on four wheels, but a real world AI system that perceives the world. Yeah. And has, can have potentially different embodiments.

2 (2h 4m 2s):
Well I mean the really wild thing about the end-to-end training is that like it learns to read. Like you can read signs, but we never taught it to read. So Yeah. We never taught it what, we never taught it what a car was or what a person was or a bi cyclist. It learnt what, what all those things are, what all the objects are on the road from video, just from watching videos. Just like humans. I mean humans are photons and control controls out like the vast majority of information reaching our brain is from our eyes.

2 (2h 4m 44s):
And you say, well, what's the output? The output is our motor signals to our sort of fingers and mouth in order to communicate protons in controls out The same is true of the car.

0 (2h 5m 1s):
But by looking at the sequence of images, it's, you've agreed with Ilyas Discover recently where he talked about LLM forming a world model and basically language is a projection of that wall model onto the sequence of Yeah. Letters and, and you saying it

2 (2h 5m 18s):
Finds order in in, in these things. It finds correlative clusters

0 (2h 5m 27s):
In so doing it's like understanding something deep about the world.

2 (2h 5m 30s):
Yeah.

0 (2h 5m 31s):
Which is like, and it is beautiful.

2 (2h 5m 36s):
That's how our brain works.

0 (2h 5m 38s):
Yeah. But it's, it's beautiful

2 (2h 5m 40s):
Protons in controls

0 (2h 5m 41s):
Out neur nets are able to understand that deep meaning in the world. And so the, the question is how far can it go? And, and it does seem everybody's excited about LLMs. So in the space of self supervised learning in the space of text, yeah. It, it seems like there's a deep similarity between that and what Tesla Autopilot is doing. Is it to you basically the same data. They're converging,

2 (2h 6m 9s):
They're converging.

0 (2h 6m 11s):
I wonder who gets there faster understanding, having a deep understanding of the world, or they just will naturally converge.

2 (2h 6m 19s):
They're both headed towards AGI The Tesla approach is much more compute efficient. It had to be 'cause we were constrained on this, this, you know, we only have a hundred watts an into eight computer, 144 trillion operations per second, which sounds like a lot, but is kind of small potatoes these days at into eight. But it's understanding the world at into eight's only 2 56 values.

0 (2h 6m 56s):
But there, the path to AGI might have much more significant impact because it's understanding it'll, it'll faster understand the real world than will LMS and therefore be able to integrate with, with the real humans in the real world faster.

2 (2h 7m 13s):
They're both going to understand the world, but I think Tesla's approach is fundamentally more computer efficient. It had to be, there was no choice. Like our brain is very computer efficient, very, very energy efficient. So think of like, what, what is our brain able to do? You know, there's only about 10 watts of higher brain function, not counting stuff that's just used to control our body. The thinking part of our brain is less than 10 watts, and that 10, those 10 watts can still produce a much better novel than a 10 megawatt GPU cluster. So there's a six order of magnitude difference there.

2 (2h 7m 56s):
I mean, the, the AI has thus far gotten to where it is, where via brute force, just throwing massive amounts of compute and, and massive amounts of power at it. So this is not where, where it'll end up, you know, in general with any given technology first try to make it work and then you make it efficient. So I think we'll find over time that these models get smaller, are, are able to do produce sensible output with far less compute. Wireless power.

2 (2h 8m 37s):
Tesla is arguably ahead of the game on that front because it has, we've just been forced to try to understand the world with a hundred watts of compute. And there are a bunch of sort of fundamental functions that we kind of forgot to include. So we have to run them in a bunch of things in emulation. We fixed, fixed a bunch of those with hardware four, and then hardware five will be even better. But I, it does appear at this point that the car will be able to drive better than a human, even with hardware three and, and a hundred watts of power.

2 (2h 9m 21s):
And really, if we really optimize it could be probably less than 50 watts.

0 (2h 9m 27s):
What have you learned about developing Optimus, about applying, integrating this kind of real world AI into the space of robotic manipulation, just humanoid robotics? What are some interesting tiny or big things you've understood?

2 (2h 9m 48s):
I was surprised at the fact that we had to develop every part of the robot ourselves. That there were no off the shelf motors, electronics sensors, like we had to develop everything. We, we couldn't, we couldn't actually find a source of electric motors for any amount of money. So

0 (2h 10m 13s):
It's not even just the, the efficient, inexpensive, it's like anything, there's not a no, huh? The actuators, everything, everything has to be,

2 (2h 10m 21s):
Yeah,

0 (2h 10m 22s):
Designed from scratch.

2 (2h 10m 23s):
We tried hard to find anything that was, because you think of how many electric motors are made in the world. There's like tens of thousands, hundreds of thousands of electric motor designs. None of them were suitable for a humanoid robot. Literally none. So we had to develop our own design design specifically for, for, for what a humanoid robot needs.

0 (2h 10m 51s):
How hard was it to design something that's can be mass manufactured? Could be relatively inexpensive. I mean, if you compare to Boston Dynamics Atlas, that's a very expensive robot.

2 (2h 11m 2s):
It is designed to be manufactured in the same way they would make a car. And I think ultimately we can make Optimus for less than the cost of a car. It should be, 'cause if you look at the mass of the robot, it's much smaller and the car has many actuators in it. The car has more actuators than the robot.

0 (2h 11m 23s):
But there is, the actuators are kind of interesting on a human, a robot with the fingers. So Optimus is really nice. Hands and fingers, you know?

2 (2h 11m 33s):
Yeah.

0 (2h 11m 34s):
And they could do some interesting manipulation, soft, soft touch robotics.

2 (2h 11m 39s):
I mean, one of the tests I ha goals I have is, can, can it pick up a needle and a thread and thread the needle just by looking how

0 (2h 11m 48s):
Far away? Away from that? Just by looking, just by

2 (2h 11m 51s):
Looking maybe a year. Hmm. Although I go back to, I'm optimistic on time. The work that we're doing in the car will translate to the robot.

0 (2h 11m 59s):
The perception or the, also the control, the,

2 (2h 12m 2s):
No, the controls are different, but the, the video in controls out the, the car is robot on four wheels. The, the, the Optimus is a robot with the hands and legs.

0 (2h 12m 15s):
So you can

2 (2h 12m 16s):
Just, but they're, they're very, they're very similar.

0 (2h 12m 18s):
So the entire machinery of the learning process, yeah. End to end is just, you just have a different set of controls.

2 (2h 12m 24s):
Optimus will figure out how to do things by watching videos.

0 (2h 12m 28s):
As the saying goes, be kind for everyone you meet is fighting a battle, you know nothing about. Yeah,

2 (2h 12m 33s):
It's

0 (2h 12m 34s):
True. What's something difficult you're going through that people don't often see

2 (2h 12m 38s):
Trying to feed Uber? No, I mean, you know, I mean, my mind is a storm and I don't, I don't think, I don't think most people would wanna be me. They may think they'd wanna be me, but they don't, they don't know. They don't understand.

0 (2h 13m 11s):
How are you doing?

2 (2h 13m 15s):
I mean, overall, okay, in the grand scheme of things, I can't complain.

0 (2h 13m 21s):
You get lonely

2 (2h 13m 24s):
Sometimes, but I, you know, my kids and friends keep me company,

0 (2h 13m 33s):
So not existential.

2 (2h 13m 36s):
There are many nights I sleep alone. I don't have to, but I do.

0 (2h 13m 46s):
Walter Isaacson, in this new biography of you wrote about your difficult childhood, will you ever find forgiveness in your heart for everything that has happened to you in that period of your life?

2 (2h 14m 1s):
What is forgiveness? I do not. At least I don't think I have a resentment, so nothing to forgive.

0 (2h 14m 20s):
You know, forgiveness is difficult for people. It seems like you don't harbor their resentment.

2 (2h 14m 29s):
I mean, I try to think about like, what, what, what is gonna affect the future in a good way. And holding onto grudges does not affect the future in a good way.

0 (2h 14m 41s):
You're a father, a proud father. What have you learned about life from your kids? Those little biological organisms?

2 (2h 14m 53s):
I mean, developing AI and watching say a little X grow is fascinating because they, there are far more parallels than I would've expected. I mean, I can see his biological neural net making more and more sense to the world. And I can see the digital neural net making more and more sense of the world at the same time.

0 (2h 15m 19s):
Do you see the beauty in magic? In both?

2 (2h 15m 22s):
Yes. I mean, one of the things with, with kids is that, you know, you, you kind of see the world anew in their eyes. You know, to them everything is new and fresh. And, and then when you, when you see that them experience, the, the world is new and fresh, you do too

0 (2h 15m 53s):
Well. Elon, I just wanna say thank you for your kindness to me and friendship over the years for seeing something in a silly kid like me, as you've done for many others. And thank you for having hope for a positive future for Humanity and for working your ass off to make it happen. Thank you, Jan. Thanks

2 (2h 16m 12s):
Lex

0 (2h 16m 13s):
Thank you for listening to this conversation with Elon Musk to support this podcast, Please, check out our sponsors in the description, And. now let me leave you with some words that Walter Isaacson wrote about the central philosophy of how Elon approaches difficult problems. The only rules are the ones dictated by the laws of Physics. Thank you for listening and hope to see you next time.